{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BOW.ipynb","provenance":[{"file_id":"16LC16BtCxDitLEXzZlj5xDswL-v6NFI3","timestamp":1600883903974}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"xZRYVm9y1eAC","executionInfo":{"status":"ok","timestamp":1601453222336,"user_tz":-210,"elapsed":3089,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["from google.colab import drive\n","import warnings"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FxZH0CrXWAlN"},"source":["**Authentication**"]},{"cell_type":"code","metadata":{"id":"y7fiSgckBIpF","executionInfo":{"status":"ok","timestamp":1601453223416,"user_tz":-210,"elapsed":4157,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"1382afa1-afdf-4400-fc5e-872c1884d88f","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["drive.mount('/gdrive', force_remount=True)\n","%cd /gdrive/My Drive/Persian-Reverse-Dictionary/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n","/gdrive/My Drive/Persian-Reverse-Dictionary\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bMzxZkFjWHca"},"source":["**Importing the necessary libraries**"]},{"cell_type":"code","metadata":{"id":"wQ224cPaBO8L","executionInfo":{"status":"ok","timestamp":1601453225301,"user_tz":-210,"elapsed":6029,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["from experiment_v8 import *\n","from globals import *\n","from attention import *"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_lup1Ja_WcPF"},"source":["**Configuration**"]},{"cell_type":"code","metadata":{"id":"CLytgx5NCDvq","executionInfo":{"status":"ok","timestamp":1601453225302,"user_tz":-210,"elapsed":6020,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["np.set_printoptions(suppress=True)\n","experiment_phase = 'querying' # phase: training/querying/continue\n","warnings.filterwarnings(\"ignore\")\n","random_seed = 1234\n","tf.random.set_seed(random_seed)\n","seed(random_seed)\n","shared_files = read_csv_as_dict('shared_files.csv')\n","globals().update(shared_files)\n","model_architecture = 'bow'\n","project_path = 'models/'+model_architecture+'/'\n","num_head_words = 20000\n","folder_path = str(num_head_words)+'/'\n","gloss_max_rank = 100000\n","input_emb_size = 300\n","lstm_output_size = 300\n","dense_output_size = 300\n","output_emb_size = 300\n","pretrained_input = True\n","fixed_embeddings = False\n","pretrained_target = True\n","normalize_vectors = False\n","add_context = False\n","augment_head = False\n","augment_gloss = False\n","save_train_test_samples = False\n","save_tools = False\n","save_weights = True\n","use_intent_classifier = False\n","max_hfake_per_sample = 5\n","gfake_per_sample = {\n","    3:5,\n","    4:5\n","}\n","context_max_rank = 100000\n","max_seq_len = 20\n","learning_rate = 1.0\n","margin = 1.0\n","num_epochs = 50\n","batch_size = 16\n","mos_eval_sample_size = 40\n","result_topn = 10\n","acc_eval_sample_Size = 500\n","input_vector_model = 'fasttext'\n","output_vector_model = 'fasttext'\n","data_sources = {'wikipedia','amid','dehkhoda-vy','moeen-vy','farsnet'}\n","vector_model_dir = {'fasttext':fasttext_dict_dir,'irblog':irblog2_wv_dir,'wiki200':wiki_wv_200_dir,'hamwv':hamshahri_wv_dir,'hamft':hamshahri_ft_dir,'wordak':wordak_wv_dir, 'wiki300':wiki_wv_300_dir,'twitter':twitter_wv_dir}\n","max_sense = {\n","    'wikipedia':500,\n","    'amid':500,\n","    'dehkhoda-vy':500,\n","    'moeen-vy':500,\n","    'farsnet':500\n","}\n","num_heads = 8\n","d_model = 300\n","loss_function = 'cosine' # cosine, ranking\n","intent_project_path = 'July/2020-7-23/'\n","intent_folder_path = 'exp9/'\n","intent_maxlen = 20\n","intent_threshold = 0.3\n","intent_coefficient = 0.4\n","vec_sim_coefficient = 0.6"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"WSYzfzFTHopD","executionInfo":{"status":"ok","timestamp":1601453225303,"user_tz":-210,"elapsed":6013,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if experiment_phase == 'training':\n","    log_dir = project_path+folder_path+'report.dat'\n","    log_obj = {\n","        'gloss':str(gloss_max_rank),\n","        'head':str(num_head_words),\n","        'input_emb':str(input_emb_size),\n","        'output_emb':str(output_emb_size),\n","        'dense_output':str(dense_output_size),\n","        'lstm_output':str(lstm_output_size),\n","        'pretrained_input':input_vector_model if pretrained_input else 'no',\n","        'pretrained_target':output_vector_model if pretrained_target else 'no',\n","        'fixed_embeddings':'yes' if fixed_embeddings else 'no',\n","        'normalize_vectors':'yes' if normalize_vectors else 'no',\n","        'intent_classifier':'yes' if use_intent_classifier else 'no',\n","        'context':str(context_max_rank) if add_context else 'no',\n","        'augment_head':str(max_hfake_per_sample) if augment_head else 'no',\n","        'augment_gloss':str(gfake_per_sample) if augment_gloss else 'no',\n","        'margin':str(margin),\n","        'seq_len':str(max_seq_len),\n","        'learning_rate':str(learning_rate),\n","        'batch_size':str(batch_size),\n","        'data_sources':\" \".join(data_sources),\n","        'max_sense':str(max_sense),\n","        'model_architecture':model_architecture,\n","        'loss_function':loss_function,\n","        'intent_path':intent_project_path+intent_folder_path,\n","        'intent_maxlen':str(intent_maxlen),\n","        'intent_threshold':str(intent_threshold),\n","        'intent_coefficient':str(intent_coefficient),\n","        'vec_sim_coefficient':str(vec_sim_coefficient)\n","    }\n","    init_log(log_obj, log_dir)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Dhnb4tXWtDP"},"source":["**Loading**<br/>\n","* Data \n","* Stopwords\n","* Normalizer\n","* Ranking of words by frequency\n","* Synonyms Set\n","* POS Tags\n","* Vector Model(s)"]},{"cell_type":"code","metadata":{"id":"4Fc3W5BaNM4-","executionInfo":{"status":"ok","timestamp":1601453230781,"user_tz":-210,"elapsed":11481,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["word_tags = read_pickle(wordtags_dir) if use_intent_classifier else defaultdict(set)\n","ranking = read_pickle(ranking_dir)\n","all_words = set(ranking)\n","stopwords = read_lines(stopwords_dir)\n","normalizer = read_json(normalizer_dir)\n","synonyms = read_pickle(synonyms_dir)\n","if experiment_phase in {'training','continue'}:\n","    data = read_pickle(data_dir)\n","input_vec_model = read_pickle(vector_model_dir[input_vector_model])\n","output_vec_model = read_pickle(vector_model_dir[input_vector_model])\n","if use_intent_classifier == True:\n","    with open(intent_project_path+intent_folder_path+'model.json','r') as json_file:\n","        intent_model_arch = json_file.read()\n","    intent_model = tf.keras.models.model_from_json(intent_model_arch)\n","    intent_model.load_weights(intent_project_path+intent_folder_path+'weights.h5')\n","    intent_id2c = read_pickle(intent_project_path+intent_folder_path+'id2c.pkl')\n","    intent_g2id = read_pickle(intent_project_path+intent_folder_path+'intent-g2id.pkl')\n","    tools = {'normalizer':normalizer,'stopwords':stopwords,'synonyms':synonyms,'word_tags':word_tags, 'intent_g2id':intent_g2id, 'intent_maxlen':intent_maxlen, 'intent_model':intent_model, 'intent_threshold':intent_threshold, 'intent_id2c':intent_id2c, 'intent_coefficient':intent_coefficient, 'vec_sim_coefficient':vec_sim_coefficient, 'output_emb_size':output_emb_size}\n","else:\n","    tools = {'normalizer':normalizer,'stopwords':stopwords,'synonyms':synonyms,'word_tags':word_tags, 'output_emb_size':output_emb_size}"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"pB24i9DjCopu","executionInfo":{"status":"ok","timestamp":1601453231317,"user_tz":-210,"elapsed":12007,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["items = []\n","sources_counter = Counter()\n","word_rank = {word:idx+1 for idx, word in enumerate(ranking)}\n","if experiment_phase in {'training','continue'}:\n","    for item in data:\n","        sources_counter[item['dic']] += 1\n","        new_item = {'word':item['word'],\n","                    'rank':word_rank[item['word']]+1 if item['word'] in all_words else len(ranking)+1,\n","                    'original_definition':item['meaning'],\n","                    'preprocessed_definition':item['meaning'],\n","                    'active':True,\n","                    'array':None,\n","                    'context':False,\n","                    'type':'main',\n","                    'sense_tags':item['tags'] if 'tags' in item else set(),\n","                    'general_tags':item['general_tags'] if 'general_tags' in item else set(),\n","                    'sid':item['sense_id'],\n","                    'source':item['dic'],\n","                    'phase':item['phase']\n","                    }\n","        items.append(new_item)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5VzY8W9_XFQh"},"source":["**Preparing the Train, Test and Dev Sets**"]},{"cell_type":"code","metadata":{"id":"B01R_HZH8WKA","executionInfo":{"status":"ok","timestamp":1601453231948,"user_tz":-210,"elapsed":12629,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if experiment_phase in {'training','continue'}:\n","    ## fixing the data sources and senses\n","    for item in items:\n","        if item['source'] not in data_sources or (item['sid']>max_sense[item['source']]):\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    ## removing html tags\n","    for item in items:\n","        item['word'] = remove_html_tags(item['word'])\n","        item['original_definition'] = remove_html_tags(item['original_definition'])\n","        item['preprocessed_definition'] = remove_html_tags(item['preprocessed_definition'])\n","    ## deactivating samples without any Persian information\n","    for item in items:\n","        if not (has_any_persian(item['word']) and has_any_persian(item['original_definition'])):\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'with Persian information - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","    ## normalizing characters\n","    for item in items:\n","        item['preprocessed_definition'] = normalize_characters(item['preprocessed_definition'], normalizer)\n","        item['word'] = normalize_characters(item['word'], normalizer)\n","    ## correcting whitespaces\n","    for item in items:\n","        item['word'] = correct_whitespaces(item['word'])\n","        item['preprocessed_definition'] = correct_whitespaces(item['preprocessed_definition'])\n","    ## tokenization\n","    for item in items:\n","        item['preprocessed_definition'] = tokenize(item['preprocessed_definition'])\n","    ## removing self-definition(s)\n","    for item in items:\n","        item['preprocessed_definition'] = remove_self_definition(item['preprocessed_definition'], item['word'])\n","    ## removing stopwords\n","    for item in items:\n","        item['preprocessed_definition'] = remove_stopwords(item['preprocessed_definition'], stopwords)\n","    for item in items:\n","        if item['word'] in stopwords:\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'Stopwords removed - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","    ## deactivating unlearnables\n","    for item in items:\n","        if not has_vector(item['word'], output_vec_model):\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'Unlearnables Removed - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","    ## deactivating short words\n","    for item in items:\n","        if len(item['word']) < 3:\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'Short words removed - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","    ## deactivating short definitions\n","    for item in items:\n","        if len(item['preprocessed_definition']) == 1 and len(item['preprocessed_definition'][0]) < 3:\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'Short definitions removed - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","    ## deactivating meaningless words\n","    for item in items:\n","        if len(item['preprocessed_definition']) == 0:\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'Meaningless words removed - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","    ## normalizing heads by ranking (frequency)\n","    active_words = set([item['word'] for item in items if item['active']==True])\n","    temp_ranking = [word for word in ranking if word in active_words]\n","    frequent_words = set([word for word in temp_ranking][:num_head_words])\n","    context_words = set([word for word in temp_ranking][:context_max_rank])\n","    for item in items:\n","        if item['word'] not in frequent_words:\n","            if add_context == True:\n","                if item['word'] in context_words:\n","                    item['phase'] = 'train'\n","                    item['context'] = True\n","                else:\n","                    item['active'] = False\n","            else:\n","                item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    main_words = set([item['word'] for item in items if item['context']==False])\n","    log_str = 'Normalized heads by frequency - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words)) +' ('+str(len(main_words))+' Main Words)'\n","    write_line_to_file(log_str, log_dir)\n","    ## generating the comparison matrix\n","    h2id = {}\n","    id2h = {}\n","    for i, h in enumerate(frequent_words):\n","        h2id[h] = i\n","        id2h[i] = h\n","    comparison_matrix = zeros((num_head_words, output_emb_size))\n","    if pretrained_target == True:\n","        for item in items:\n","            if item['phase'] == 'train' and item['word'] in frequent_words:\n","                comparison_matrix[h2id[item['word']]] = output_vec_model[item['word']]\n","    ## normalizing the tokens based on their frequency\n","    tokens_lst = []\n","    for item in items:\n","        if item['phase'] == 'train':\n","            tokens_lst.extend(item['preprocessed_definition'])\n","    frequent_tokens = most_frequent(tokens_lst, gloss_max_rank)\n","    for item in items:\n","        item['preprocessed_definition'] = [token if token in frequent_tokens else 'UNK' for token in item['preprocessed_definition']]\n","        if 'UNK' in item['preprocessed_definition']:\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    main_words = set([item['word'] for item in items if item['context']==False])\n","    log_str = 'Normalized tokens by frequency - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words)) +' ('+str(len(main_words))+' Main Words)'\n","    write_line_to_file(log_str, log_dir)\n","    ## encoding the tokens\n","    t2id = {}\n","    id2t = {}\n","    t2id['PAD'] = 0\n","    t2id['UNK'] = 1\n","    id2t[0] = 'PAD'\n","    id2t[1] = 'UNK' \n","    for i, t in enumerate(frequent_tokens, start=2):\n","        t2id[t] = i\n","        id2t[i] = t\n","    ## generating the embedding matrix\n","    embedding_matrix = zeros((len(frequent_tokens)+2, input_emb_size))\n","    if pretrained_input == True:\n","        for token in frequent_tokens:\n","            try:\n","                embedding_matrix[t2id[token]] = input_vec_model[token]\n","                if normalize_vectors == True and LA.norm(embedding_matrix[t2id[token]])>0.0:\n","                    embedding_matrix[t2id[token]] = embedding_matrix[t2id[token]]/LA.norm(embedding_matrix[t2id[token]])\n","            except:\n","                continue\n","\n","    ## fixing the length of sequences\n","    for item in items:\n","        tokens = item['preprocessed_definition']\n","        item['preprocessed_definition'] = tokens[:max_seq_len] if len(tokens)>=max_seq_len else tokens+['PAD' for j in range(max_seq_len-len(tokens))]\n","    ## removing the words with definitions that are only consisted of 'PAD' and 'UNK'\n","    for item in items:\n","        tokens = item['preprocessed_definition']\n","        tokens = set(tokens)\n","        if tokens.issubset({'PAD','UNK'}):\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'Infoless definitions (UNK and PAD) removed - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","\n","    ## removing duplicates (samples with equal word and definition)\n","    # from the training data\n","    sample_count = Counter()\n","    for item in items:\n","        if item['phase'] == 'train':\n","            word = item['word']\n","            tokens = item['preprocessed_definition']\n","            sample_key = word+'-'+\"-\".join(tokens)\n","            sample_count[sample_key] += 1\n","            if sample_count[sample_key] > 1:\n","                item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    main_words = set([item['word'] for item in items if item['context']==False])\n","    log_str = 'duplicates removed from training data - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))+' ('+str(len(main_words))+' Main Words)'\n","    write_line_to_file(log_str, log_dir)\n","    # from the testing and development data\n","    for item in items:\n","        if item['phase'] != 'train':\n","            word = item['word']\n","            tokens = item['preprocessed_definition']\n","            sample_key = word+'-'+\"-\".join(tokens)\n","            sample_count[sample_key] += 1\n","            if sample_count[sample_key] > 1:\n","                item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    main_words = set([item['word'] for item in items if item['context']==False])\n","    log_str = 'duplicates removed from unseen data - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))+' ('+str(len(main_words))+' Main Words)'\n","    write_line_to_file(log_str, log_dir)\n","    ## generating the data\n","    for item in items:\n","        tokens = item['preprocessed_definition']\n","        nparray = [t2id[token] for token in tokens]\n","        nparray = array(nparray)\n","        item['array'] = nparray\n","    train = [item for item in items if item['phase'] == 'train']\n","    test = [item for item in items if item['phase'] == 'test']\n","    dev = [item for item in items if item['phase'] == 'dev']\n","    log_str = 'After Preprocessing: '+str(len(train))+' training samples, '+str(len(test))+' testing samples and '+str(len(dev))+' samples for development'\n","    write_line_to_file(log_str, log_dir)\n","    tools_part2 = {'t2id':t2id,'id2t':id2t,'h2id':h2id,'id2h':id2h,'embedding_matrix':embedding_matrix,'comparison_matrix':comparison_matrix}\n","    tools.update(tools_part2)\n","    train_sources_counter = Counter()\n","    for item in train:\n","        train_sources_counter[item['source']] += 1\n","    test_sources_counter = Counter()\n","    for item in test:\n","        test_sources_counter[item['source']] += 1\n","    dev_sources_counter = Counter()\n","    for item in dev:\n","        dev_sources_counter[item['source']] += 1\n","    write_line_to_file('Training Data: '+str(sum(train_sources_counter.values())), log_dir)\n","    write_line_to_file(str(train_sources_counter), log_dir)\n","    write_line_to_file('Development Data: '+str(sum(dev_sources_counter.values())), log_dir)\n","    write_line_to_file(str(dev_sources_counter), log_dir)\n","    write_line_to_file('Testing Data: '+str(sum(test_sources_counter.values())), log_dir)\n","    write_line_to_file(str(test_sources_counter), log_dir)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"XOsqxIYlGMZ7","executionInfo":{"status":"ok","timestamp":1601453231949,"user_tz":-210,"elapsed":12621,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["# if experiment_phase == 'training':\n","#     words = list(set(list(h2id.keys())))\n","#     with open(project_path+folder_path+'words'+str(num_head_words)+'.pkl','wb') as file:\n","#         pickle.dump(words, file)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRtgkwzAWtGn","executionInfo":{"status":"ok","timestamp":1601453231950,"user_tz":-210,"elapsed":12612,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if experiment_phase in {'training','continue'}:\n","    from copy import deepcopy\n","    # Making a counter to see if a sample already exists\n","    item_count = Counter()\n","    for item in items:\n","        word = item['word']\n","        tokens = item['preprocessed_definition']\n","        definition = \" \".join(tokens)\n","        key = word+'-'+definition\n","        item_count[key] += 1\n","\n","    # Augmenting (head words)\n","\n","    head_augment = []\n","    all_heads = set([item['word'] for item in items if item['phase']=='train'])\n","    replaceable_heads = set([word for word in all_heads if word in synonyms])\n","    if augment_head == True:\n","        for item in train:\n","            if len(train) % 100000 == 0:\n","                print(len(train))\n","            word = item['word']\n","            tokens = item['preprocessed_definition']\n","            definition = \" \".join(tokens)\n","            syn_words = synonyms[word]\n","            syn_words = set([w for w in syn_words if w in output_vec_model and w != word])\n","            if len(syn_words) == 0:\n","                continue\n","            if len(syn_words) >= max_hfake_per_sample:\n","                syn_candidates = sample(syn_words, max_hfake_per_sample)\n","            else:\n","                syn_candidates = list(syn_words)\n","            for candidate in syn_candidates:\n","                new_item = item.copy()\n","                new_item['word'] = candidate\n","                new_item['type'] = 'augment'\n","                key = candidate+'-'+definition\n","                if item_count[key] == 0:\n","                    head_augment.append(new_item)\n","                    item_count[key] += 1\n","\n","    # Augmenting (gloss words)\n","\n","    gloss_augment = []\n","\n","\n","    usable_synonyms = defaultdict()\n","    for (word, synset) in synonyms.items():\n","        temp_synset = set([syn for syn in synset if syn != word and syn in t2id])\n","        if len(temp_synset) > 0:\n","            usable_synonyms[word] = temp_synset\n","\n","    gloss_augment = []\n","\n","    if augment_gloss == True:\n","        for item in train:\n","            \n","            if item['type'] != 'main':\n","                continue\n","            word = item['word']\n","            tokens = item['preprocessed_definition']\n","            definition = \" \".join(tokens)\n","            rep_tokens = set([token for token in tokens if token in usable_synonyms and token in input_vec_model])\n","            for num_tokens_to_replace in gfake_per_sample.keys():\n","                if len(rep_tokens) < num_tokens_to_replace:\n","                    continue\n","                remaining_samples = gfake_per_sample[num_tokens_to_replace]\n","                remaining_tries = 50\n","                while remaining_samples > 0 and remaining_tries > 0:\n","                    chosen_to_be_changed = set(sample(rep_tokens, num_tokens_to_replace))\n","                    what_to_replace_with = defaultdict()\n","                    for token in chosen_to_be_changed:\n","                        candidates = usable_synonyms[token]\n","                        what_to_replace_with[token] = sample(candidates, 1)[0]\n","                    new_item = deepcopy(item)\n","                    new_item['preprocessed_definition'] = [what_to_replace_with[t] if t in chosen_to_be_changed else t for t in new_item['preprocessed_definition']]\n","                    new_item['array'] = np.array([t2id[t] for t in new_item['preprocessed_definition']])\n","                    new_item_key = new_item['word'] + '-' + \" \".join(new_item['preprocessed_definition'])\n","                    remaining_tries -= 1\n","                    if item_count[new_item_key] == 0:\n","                        gloss_augment.append(new_item)\n","                        item_count[new_item_key] += 1\n","                        remaining_samples -= 1\n","\n","\n","    train.extend(head_augment)\n","    del head_augment\n","    log_str = 'finished augmentation (head) - '+str(len(train))+' samples'\n","    write_line_to_file(log_str, log_dir)\n","    train.extend(gloss_augment)\n","    del gloss_augment\n","    log_str = 'finished augmentation (gloss) - '+str(len(train))+' samples'\n","    write_line_to_file(log_str, log_dir)\n","    del items"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"JDRxe3_DeGMn","executionInfo":{"status":"ok","timestamp":1601453231951,"user_tz":-210,"elapsed":12604,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if experiment_phase == 'training' and save_train_test_samples == True:\n","    train_data = defaultdict(list)\n","    test_data = defaultdict(list)\n","    train_original_data = defaultdict(list)\n","    test_original_data = defaultdict(list)\n","    for item in train:\n","        word = item['word']\n","        original_definition = \" \".join(item['original_definition'].split()[:max_seq_len])\n","        definition = \" \".join([token for token in item['preprocessed_definition'] if not token == 'PAD'])\n","        train_data[word].append(definition)\n","        train_original_data[word].append(original_definition)\n","    for item in test:\n","        word = item['word']\n","        original_definition = \" \".join(item['original_definition'].split()[:max_seq_len])\n","        definition = \" \".join([token for token in item['preprocessed_definition'] if not token == 'PAD'])\n","        test_data[word].append(definition)\n","        test_original_data[word].append(original_definition)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"exyXpM20Sh7Y","executionInfo":{"status":"ok","timestamp":1601453231951,"user_tz":-210,"elapsed":12596,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if experiment_phase in {'training','continue'}:\n","    x_train = array([item['array'] for item in train])\n","    y_train = array([comparison_matrix[h2id[item['word']]] if item['word'] in h2id else output_vec_model[item['word']] for item in train])\n","    x_dev = array([item['array'] for item in dev])\n","    y_dev = array([comparison_matrix[h2id[item['word']]] if item['word'] in h2id else output_vec_model[item['word']] for item in dev])"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qTHzcwVEXQ3k"},"source":["**Saving the necessary variables**"]},{"cell_type":"code","metadata":{"id":"CAMMkkIVXR6V","executionInfo":{"status":"ok","timestamp":1601453231953,"user_tz":-210,"elapsed":12589,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if experiment_phase == 'training':\n","    if save_tools == True:\n","        with open(project_path+folder_path+'tools.pkl','wb') as file:\n","            pickle.dump(tools, file)\n","    if save_train_test_samples == True:\n","        train_test_data = (train_data, train_original_data, test_data, test_original_data)\n","        with open(project_path+folder_path+'train_test_data.pkl','wb') as file:\n","            pickle.dump(train_test_data, file)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"mx7KX9xvKhPi","executionInfo":{"status":"ok","timestamp":1601453231953,"user_tz":-210,"elapsed":12580,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if experiment_phase == 'training':\n","    emb_matrix_shape = embedding_matrix.shape\n","    additional_tools = {'comparison_matrix':comparison_matrix, 't2id':t2id, 'h2id':h2id, 'id2h':id2h}\n","    query_tools = (emb_matrix_shape, additional_tools)\n","    with open(project_path+folder_path+'query_tools.pkl','wb') as file:\n","        pickle.dump(query_tools, file)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F_gBuN56Ivew"},"source":["**Loading Necessary Tools for Querying**"]},{"cell_type":"code","metadata":{"id":"bWelxKZ2KJeJ","executionInfo":{"status":"ok","timestamp":1601453233842,"user_tz":-210,"elapsed":14461,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if experiment_phase == 'querying':\n","    emb_matrix_shape , additional_tools = read_pickle(project_path+folder_path+'query_tools.pkl')\n","    tools.update(additional_tools)"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WsWo-X61Xiku"},"source":["**Making a Model and Training it**"]},{"cell_type":"markdown","metadata":{"id":"16aucIuAj_Gy"},"source":["*Bag of Words*"]},{"cell_type":"code","metadata":{"id":"pRZhlJhpiipI","executionInfo":{"status":"ok","timestamp":1601453234251,"user_tz":-210,"elapsed":14854,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if model_architecture == 'bow':\n","    sequence_input = Input(shape=(max_seq_len,), dtype='int32', name='input')\n","    embedding_layer = Embedding(emb_matrix_shape[0],\n","                                input_emb_size,\n","                                weights=[embedding_matrix] if 'embedding_matrix' in globals() else [np.zeros(emb_matrix_shape)],\n","                                input_length=max_seq_len,\n","                                trainable=not fixed_embeddings,\n","                                mask_zero=True, name='embedding')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    mean_vector = Lambda(lambda x: mean(x, axis=1), name='lambda')(embedded_sequences)\n","    mean_vector = Flatten(name='flatten')(mean_vector)\n","    output = Dense(dense_output_size, activation='tanh', name='dense')(mean_vector)\n","    adadelta = optimizers.Adadelta(lr=learning_rate)\n","    model = Model(inputs=sequence_input, outputs=output)\n","    if loss_function == 'cosine':\n","        model.compile(loss=cosine_loss,\n","                    optimizer=adadelta,metrics=[cosine_sim])\n","    elif loss_function == 'ranking':\n","        model.compile(loss=rank_loss(margin),\n","                    optimizer=adadelta,metrics=[cosine_sim])"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2o1UORkqk6Ed"},"source":["*LSTM (without attention)*"]},{"cell_type":"code","metadata":{"id":"hGT_QZftoYWw","executionInfo":{"status":"ok","timestamp":1601453234252,"user_tz":-210,"elapsed":14847,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if model_architecture == 'lstm':\n","    sequence_input = Input(shape=(max_seq_len,), dtype='int32', name='input')\n","    embedding_layer = Embedding(emb_matrix_shape[0],\n","                                input_emb_size,\n","                                weights=[embedding_matrix]  if 'embedding_matrix' in globals() else [np.zeros(emb_matrix_shape)],\n","                                input_length=max_seq_len,\n","                                trainable=not fixed_embeddings,\n","                                mask_zero=True, name='embedding')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    lstm = LSTM(lstm_output_size, return_sequences=False, name='lstm')(embedded_sequences)\n","    output = Dense(dense_output_size, activation='tanh', name='dense')(lstm)\n","    adadelta = optimizers.Adadelta(lr=learning_rate)\n","    model = Model(inputs=sequence_input, outputs=output)\n","    if loss_function == 'cosine':\n","        model.compile(loss=cosine_loss,\n","                    optimizer=adadelta,metrics=[cosine_sim])\n","    elif loss_function == 'ranking':\n","        model.compile(loss=rank_loss(margin),\n","                    optimizer=adadelta,metrics=[cosine_sim])"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FkWxjSIZlwAA"},"source":["*BiLSTM (without attention)*"]},{"cell_type":"code","metadata":{"id":"6SWCnEdXlyU0","executionInfo":{"status":"ok","timestamp":1601453234253,"user_tz":-210,"elapsed":14839,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if model_architecture == 'bilstm':\n","    sequence_input = Input(shape=(max_seq_len,), dtype='int32', name='input')\n","    embedding_layer = Embedding(emb_matrix_shape[0],\n","                                input_emb_size,\n","                                weights=[embedding_matrix] if 'embedding_matrix' in globals() else [np.zeros(emb_matrix_shape)],\n","                                input_length=max_seq_len,\n","                                trainable=not fixed_embeddings,\n","                                mask_zero=True, name='embedding')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    lstm = Bidirectional(LSTM(lstm_output_size, return_sequences=False, name='lstm'))(embedded_sequences)\n","    output = Dense(dense_output_size, activation='tanh', name='dense')(lstm)\n","    adadelta = optimizers.Adadelta(lr=learning_rate)\n","    model = Model(inputs=sequence_input, outputs=output)\n","    if loss_function == 'cosine':\n","        model.compile(loss=cosine_loss,\n","                    optimizer=adadelta,metrics=[cosine_sim])\n","    elif loss_function == 'ranking':\n","        model.compile(loss=rank_loss(margin),\n","                    optimizer=adadelta,metrics=[cosine_sim])"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CBxsYqtk1z6J"},"source":["*LSTM (with attention)*"]},{"cell_type":"code","metadata":{"id":"N_kTQtT2Wkmk","executionInfo":{"status":"ok","timestamp":1601453234254,"user_tz":-210,"elapsed":14832,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if model_architecture == 'lstmatt':\n","    sequence_input = Input(shape=(max_seq_len,), dtype='int32', name='input')\n","    embedding_layer = Embedding(emb_matrix_shape[0],\n","                                input_emb_size,\n","                                weights=[embedding_matrix] if 'embedding_matrix' in globals() else [np.zeros(emb_matrix_shape)],\n","                                input_length=max_seq_len,\n","                                trainable=not fixed_embeddings,\n","                                mask_zero=True, name='embedding')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    lstm, state_h, state_c = LSTM(lstm_output_size, return_sequences=True, return_state=True, name='lstm')(embedded_sequences)\n","    state_h = Lambda(lambda x: tf.expand_dims(x, 1), name='expand_dims')(state_h)\n","    state_h = Dense(300, name='d1')(state_h)\n","    lstm = Dense(300, name='d2')(lstm)\n","    weights, context_vector = AdditiveAttention(name='attention')([state_h, lstm])\n","    weights = Lambda(lambda x: tf.squeeze(x), name='squeezed_weights')(weights)\n","    output = Dense(dense_output_size, activation='tanh', name='dense')(context_vector)\n","    output = Lambda(lambda x: tf.squeeze(x), name='squeezed_output')(output)\n","    adadelta = optimizers.Adadelta(lr=learning_rate)\n","    model = Model(inputs=sequence_input, outputs=output)\n","    attention_model = Model(inputs=sequence_input, outputs=weights)\n","    if loss_function == 'cosine':\n","        model.compile(loss=cosine_loss,\n","                    optimizer=adadelta,metrics=[cosine_sim])\n","    elif loss_function == 'ranking':\n","        model.compile(loss=rank_loss(margin),\n","                    optimizer=adadelta,metrics=[cosine_sim])"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bqiOp6yVl_Fe"},"source":["*BiLSTM (with attention)*"]},{"cell_type":"code","metadata":{"id":"z_CcCLEK0r8O","executionInfo":{"status":"ok","timestamp":1601453234254,"user_tz":-210,"elapsed":14822,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if model_architecture == 'bilstmatt':\n","    sequence_input = Input(shape=(max_seq_len,), dtype='int32', name='input')\n","    embedding_layer = Embedding(emb_matrix_shape[0],\n","                                input_emb_size,\n","                                weights=[embedding_matrix] if 'embedding_matrix' in globals() else [np.zeros(emb_matrix_shape)],\n","                                input_length=max_seq_len,\n","                                trainable=not fixed_embeddings,\n","                                mask_zero=True, name='embedding')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(lstm_output_size, return_sequences=True, return_state=True, name='lstm'))(embedded_sequences)\n","    state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n","    state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n","    state_h = tf.expand_dims(state_h, 1)\n","    state_h = Dense(300, name='d1')(state_h)\n","    lstm = Dense(300, name='d2')(lstm)\n","    weights, context_vector = AdditiveAttention()([state_h, lstm])\n","    weights = tf.squeeze(weights)\n","    output = Dense(dense_output_size, activation='tanh', name='dense')(context_vector)\n","    output = tf.squeeze(output)\n","    adadelta = optimizers.Adadelta(lr=learning_rate)\n","    model = Model(inputs=sequence_input, outputs=output)\n","    attention_model = Model(inputs=sequence_input, outputs=weights)\n","    if loss_function == 'cosine':\n","        model.compile(loss=cosine_loss,\n","                    optimizer=adadelta,metrics=[cosine_sim])\n","    elif loss_function == 'ranking':\n","        model.compile(loss=rank_loss(margin),\n","                    optimizer=adadelta,metrics=[cosine_sim])"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VUXgFCzbhTo","executionInfo":{"status":"ok","timestamp":1601453234255,"user_tz":-210,"elapsed":14813,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["# lstm: [2, 20, 600]\n","# state_h: [2, 600]\n","# new_lstm (after Dense(300)):  [2, 20, 300]\n","# new_state_h (after expand_dims): [2, 1, 600]\n","# new_state_h (after Dense(300)): [2, 1, 300]"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"68oE-F-AQFw3"},"source":["*Multi-head Attention*"]},{"cell_type":"code","metadata":{"id":"YFDwQjGbSvs3","executionInfo":{"status":"ok","timestamp":1601453234256,"user_tz":-210,"elapsed":14803,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if model_architecture == 'bilstmmultiatt':\n","    sequence_input = Input(shape=(max_seq_len,), dtype='int32', name='input')\n","    embedding_layer = Embedding(emb_matrix_shape[0],\n","                                input_emb_size,\n","                                weights=[embedding_matrix] if 'embedding_matrix' in globals() else [np.zeros(emb_matrix_shape)],\n","                                input_length=max_seq_len,\n","                                trainable=not fixed_embeddings,\n","                                mask_zero=True, name='embedding')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(lstm_output_size, return_sequences=True, return_state=True, name='lstm'))(embedded_sequences)\n","    state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n","    state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n","    state_h = tf.expand_dims(state_h, 1)\n","    context_vector = MultiheadAttention(num_heads, d_model)([state_h, lstm, lstm])\n","    output = Dense(dense_output_size, activation='tanh', name='dense')(context_vector)\n","    output = tf.squeeze(output)\n","    adadelta = optimizers.Adadelta(lr=learning_rate)\n","    model = Model(inputs=sequence_input, outputs=output)\n","    if loss_function == 'cosine':\n","        model.compile(loss=cosine_loss,\n","                    optimizer=adadelta,metrics=[cosine_sim])\n","    elif loss_function == 'ranking':\n","        model.compile(loss=rank_loss(margin),\n","                    optimizer=adadelta,metrics=[cosine_sim])"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"6XAA9tjEwT09","executionInfo":{"status":"ok","timestamp":1601453234256,"user_tz":-210,"elapsed":14794,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if experiment_phase == 'training':\n","    with open(log_dir,'a+',encoding='utf-8') as file:\n","        model.summary(print_fn=lambda line: file.write(line + '\\n'))\n","    write_line_to_file(str(model.loss), log_dir)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"VtLV5GUxUsOW","executionInfo":{"status":"ok","timestamp":1601453234258,"user_tz":-210,"elapsed":14787,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if experiment_phase in {'training','continue'}:\n","    json_logging_callback = LambdaCallback(on_epoch_end=lambda epoch, logs:e_end('cosine_sim', epoch, logs, project_path+folder_path+'report.dat'))\n","    optimizer_weights_callback = LambdaCallback(on_epoch_end=save_optimizer_state(model, project_path+folder_path+'optimizer_weights.pkl'))\n","    model_weights_checkpoint = ModelCheckpoint(project_path+folder_path+'weights.h5', monitor='val_loss', verbose=0,save_best_only=True, mode='min', period=1, save_weights_only=True)\n","    early_stopping = EarlyStopping(monitor='val_loss', restore_best_weights=True,patience=2, mode='min',min_delta=0.001)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"RWDiArvXU4sL","executionInfo":{"status":"ok","timestamp":1601453237576,"user_tz":-210,"elapsed":18096,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if experiment_phase == 'continue':\n","    model.fit(x_train[:1000], y_train[:1000], epochs=1, batch_size=batch_size, validation_data=(x_dev, y_dev))\n","    with open(project_path+folder_path+'optimizer_weights.pkl') as f:\n","        optimizer_weights = pickle.load(f)\n","    model.optimizer.set_weights(optimizer_weights)   \n","    model.set_weights(project_path+folder_path+'weights.h5')\n","    start = time()\n","    history = model.fit(x_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(x_dev, y_dev)\n","    ,callbacks=[model_weights_checkpoint, early_stopping, json_logging_callback, optimizer_weights_callback]\n","    )   \n","    end = time()\n","    duration = round(end-start,2)\n","    write_line_to_file('Training Duration: '+str(duration)+' Seconds', log_dir)\n","elif experiment_phase == 'training':\n","    start = time()\n","    history = model.fit(x_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(x_dev, y_dev)\n","    ,callbacks=[model_weights_checkpoint, early_stopping, json_logging_callback, optimizer_weights_callback]\n","    )   \n","    end = time()\n","    duration = round(end-start,2)\n","    write_line_to_file('Training Duration: '+str(duration)+' Seconds', log_dir)\n","elif experiment_phase == 'querying':\n","    model.load_weights(project_path+folder_path+'weights.h5')"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"HvCSOfTFPRWw","executionInfo":{"status":"ok","timestamp":1601453237578,"user_tz":-210,"elapsed":18091,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["# model.optimizer.set_weights(weight_values)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"r6Jx4uwK67RB","executionInfo":{"status":"ok","timestamp":1601453237578,"user_tz":-210,"elapsed":18083,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if experiment_phase in {'training','continue'}:\n","    x_test = array([item['array'] for item in test])\n","    y_test = array([comparison_matrix[h2id[item['word']]] if item['word'] in h2id else output_vec_model[item['word']] for item in test])\n","    test_eval = model.evaluate(x_test, y_test)\n","    test_loss = test_eval[0]\n","    test_cosim = test_eval[1]\n","    log_str = 'Test Loss: '+str(test_loss)+' - '+'Cosine Similarity: ' + str(test_cosim)\n","    write_line_to_file(log_str, log_dir)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"lt0qyeJDha_J","executionInfo":{"status":"ok","timestamp":1601453237579,"user_tz":-210,"elapsed":18076,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if experiment_phase in {'training','continue'}:\n","    plt.plot(history.history['cosine_sim'])\n","    plt.plot(history.history['val_cosine_sim'])\n","    plt.title('Training History')\n","    plt.ylabel('Cosine Similarity')\n","    plt.xlabel('epoch')\n","    plt.legend(['training', 'development'], loc='upper left')\n","    plt.show()"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"nTtfuWPQaXQP","executionInfo":{"status":"ok","timestamp":1601453237580,"user_tz":-210,"elapsed":18067,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if experiment_phase in {'training','continue'}:\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('Training History')\n","    plt.ylabel('Loss')\n","    plt.xlabel('epoch')\n","    plt.legend(['training', 'development'], loc='upper left')\n","    plt.show()"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v1NVpDTFX4MC"},"source":["**Evaluating the model based on its performance on the testing set**"]},{"cell_type":"code","metadata":{"id":"9uagVHoMKs49","executionInfo":{"status":"ok","timestamp":1601453237581,"user_tz":-210,"elapsed":18059,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if experiment_phase in {'training','continue'}:\n","    train_samples = stratified_sample(train, {'amid','moeen-vy','dehkhoda-vy'}, num_head_words, acc_eval_sample_Size)\n","    train_inputs = {\n","        'definitions':[item['original_definition'] for item in train_samples],\n","        'words':[item['word'] for item in train_samples],\n","        'sources':[item['source'] for item in train_samples],\n","        'sense_tags':[item['sense_tags'] for item in train_samples],\n","        'general_tags':[item['general_tags'] for item in train_samples]\n","    }\n","    if 'attention_model' in globals():\n","        q = query(train_inputs, result_topn, max_seq_len, {'model':model, 'attention_model':attention_model}, tools)\n","    else:\n","        q = query(train_inputs, result_topn, max_seq_len, {'model':model}, tools)\n","    if use_intent_classifier == True:\n","        write_line_to_file('-------------', log_dir)\n","        intent_eval = evaluate_intent_classifier(q, word_tags)\n","        write_line_to_file('Intent Classifier Seen Evaluation: ', log_dir)\n","        write_line_to_file(str(intent_eval), log_dir)\n","    e = evaluate(q)\n","    write_line_to_file('-------------', log_dir)\n","    log_str = 'Seen Evaluation:\\n'+'Median: '+str(e['main_eval']['median'])+', Variance: '+str(e['main_eval']['variance'])+', Acc@10: '+str(e['main_eval']['acc@10'])+', Acc@100: '+str(e['main_eval']['acc@100'])+'\\n'\n","    write_line_to_file(log_str, log_dir)\n","    log_str = 'Seen Synonym Evaluation:\\n'+'Median: '+str(e['synset_eval']['median'])+', Variance: '+str(e['synset_eval']['variance'])+', Acc@10: '+str(e['synset_eval']['acc@10'])+', Acc@100: '+str(e['synset_eval']['acc@100'])+'\\n'\n","    write_line_to_file(log_str, log_dir)\n","    write_line_to_file(str(e['bad_results']), log_dir)"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"DkFrpE5Vzm7w","executionInfo":{"status":"ok","timestamp":1601453237582,"user_tz":-210,"elapsed":18052,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if experiment_phase in {'training','continue'}:\n","    test_samples = stratified_sample(test, {'amid','moeen-vy','dehkhoda-vy'}, num_head_words, acc_eval_sample_Size)\n","    test_inputs = {\n","        'definitions':[item['original_definition'] for item in test_samples],\n","        'words':[item['word'] for item in test_samples],\n","        'sources':[item['source'] for item in test_samples],\n","        'sense_tags':[item['sense_tags'] for item in test_samples],\n","        'general_tags':[item['general_tags'] for item in test_samples]\n","    }\n","    if 'attention_model' in globals():\n","        q = query(test_inputs, result_topn, max_seq_len, {'model':model, 'attention_model':attention_model}, tools)\n","    else:\n","        q = query(test_inputs, result_topn, max_seq_len, {'model':model}, tools)\n","    if use_intent_classifier == True:\n","        write_line_to_file('-------------', log_dir)\n","        intent_eval = evaluate_intent_classifier(q, word_tags)\n","        write_line_to_file('Intent Classifier Unseen Evaluation: ', log_dir)\n","        write_line_to_file(str(intent_eval), log_dir)\n","    e = evaluate(q)\n","    write_line_to_file('-------------', log_dir)\n","    log_str = 'Unseen Evaluation:\\n'+'Median: '+str(e['main_eval']['median'])+', Variance: '+str(e['main_eval']['variance'])+', Acc@10: '+str(e['main_eval']['acc@10'])+', Acc@100: '+str(e['main_eval']['acc@100'])+'\\n'\n","    write_line_to_file(log_str, log_dir)\n","    log_str = 'Unseen Synonym Evaluation:\\n'+'Median: '+str(e['synset_eval']['median'])+', Variance: '+str(e['synset_eval']['variance'])+', Acc@10: '+str(e['synset_eval']['acc@10'])+', Acc@100: '+str(e['synset_eval']['acc@100'])+'\\n'\n","    write_line_to_file(log_str, log_dir)\n","    write_line_to_file(str(e['bad_results']), log_dir)"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y9TwgJ7hFYIv","executionInfo":{"status":"ok","timestamp":1601453237582,"user_tz":-210,"elapsed":18043,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["if experiment_phase in {'training','continue'}:\n","    synset_good10 = [item for item in q if 1<=item['synset_word_rank']<=10]\n","    synset_good100 = [item for item in q if 10<item['synset_word_rank']<=100]\n","    synset_bad100 = [item for item in q if 100<item['synset_word_rank']]\n","    save_group_reports(synset_good10, project_path+folder_path+'good10.html')\n","    save_group_reports(synset_good100, project_path+folder_path+'good100.html')\n","    save_group_reports(synset_bad100, project_path+folder_path+'bad100.html')"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"xA2Ixg6vevvl","executionInfo":{"status":"ok","timestamp":1601453237583,"user_tz":-210,"elapsed":18036,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}}},"source":["# if experiment_phase in {'training','continue'}:\n","#     dictionaries = {'amid','moeen-vy','dehkhoda-vy'}\n","#     for dictionary in dictionaries:\n","#         test_samples = stratified_sample(test, {dictionary}, num_head_words, mos_eval_sample_size)\n","#         test_inputs = {\n","#             'definitions':[item['original_definition'] for item in test_samples],\n","#             'words':[item['word'] for item in test_samples],\n","#             'sources':[item['source'] for item in test_samples]\n","#         }\n","#         if 'attention_model' in globals():\n","#             q = query(test_inputs, result_topn, max_seq_len, {'model':model, 'attention_model':attention_model}, tools)\n","#         else:\n","#             q = query(test_inputs, result_topn, max_seq_len, {'model':model}, tools)\n","#         e = evaluate(q)\n","#         write_line_to_file('-------------', log_dir)\n","#         log_str = 'Source: '+dictionary\n","#         write_line_to_file(log_str, log_dir)\n","#         log_str = 'Unseen Evaluation:\\n'+'Median: '+str(e['main_eval']['median'])+', Variance: '+str(e['main_eval']['variance'])+', Acc@10: '+str(e['main_eval']['acc@10'])+', Acc@100: '+str(e['main_eval']['acc@100'])+'\\n'\n","#         write_line_to_file(log_str, log_dir)\n","#         log_str = 'Unseen Synonym Evaluation:\\n'+'Median: '+str(e['synset_eval']['median'])+', Variance: '+str(e['synset_eval']['variance'])+', Acc@10: '+str(e['synset_eval']['acc@10'])+', Acc@100: '+str(e['synset_eval']['acc@100'])+'\\n'\n","#         write_line_to_file(log_str, log_dir)\n","#         write_line_to_file(str(e['bad_results']), log_dir)\n","#         correct_words = [item['main_word'] for item in q]\n","#         input_descriptions = [item['original_definition'] for item in q]\n","#         dict_evaluation_df = pd.DataFrame(list(zip(correct_words, input_descriptions)), columns=['word','description'])\n","#         dict_evaluation_df.to_excel(project_path+folder_path+dictionary+'.xlsx', encoding='utf-8')\n","#         for k in range(3):\n","#             output_words = [item['topwords'][k] for item in q]\n","#             evaluation_df = pd.DataFrame(list(zip(output_words, input_descriptions)), columns=['word','description'])\n","#             evaluation_df.to_excel(project_path+folder_path+dictionary+'-'+str(k+1)+'.xlsx', encoding='utf-8')"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y_w7rs5WMu3Z","executionInfo":{"status":"ok","timestamp":1601453237584,"user_tz":-210,"elapsed":18029,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"02018886-a8ed-4f23-a169-4f8abd9d05b4","colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["definition = 'حس خوشبختی که یک انسان دارد'\n","inputs = {'definitions': [definition],\n","          'words':['UNK']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: UNK</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: حس خوشبختی که یک انسان دارد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspحس&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspخوشبختی&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspیک&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspانسان&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: احساس -- آرامش -- حسی -- ناراحتی -- ذهنیت -- درد -- لذت -- عواطف -- ترس -- تنفر</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه UNK را در رتبه 20000 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (UNK) را در رتبه 20000 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"sDoYe_xQCsJw","executionInfo":{"status":"ok","timestamp":1601453237585,"user_tz":-210,"elapsed":18016,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"c65dec9e-7cfc-4612-faab-18809e4686ce","colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["definition = 'ورزشی که بیشترین طرفدار را دارد'\n","inputs = {'definitions': [definition],\n","          'words':['فوتبال']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: فوتبال</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: ورزشی که بیشترین طرفدار را دارد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspورزشی&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspبیشترین&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspطرفدار&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: فوتبال -- بسکتبال -- استادیوم -- بوکس -- لیگ -- حریف -- بازیکن -- کشتی‌گیران -- دوومیدانی -- ژیمناستیک</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه فوتبال را در رتبه 1 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (فوتبال) را در رتبه 1 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"43xGWKN3D4XT","executionInfo":{"status":"ok","timestamp":1601453396406,"user_tz":-210,"elapsed":765,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"35849fa6-cb1d-4f62-c986-f155d2488706","colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["definition = 'کسی که حالش خوش نیست'\n","inputs = {'definitions': [definition],\n","          'words':['بیمار']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: بیمار</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: کسی که حالش خوش نیست</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspکسی&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspحالش&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspخوش&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspنیست&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: دلنشین -- خندان -- بدبخت -- دلگیر -- رنجور -- ناراحت -- دلتنگ -- بیچاره -- خوشگل -- دیوانه</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه رنجور را در رتبه 5 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (بیمار) را در رتبه 425 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"Jt-SCPwiC_Ha","executionInfo":{"status":"ok","timestamp":1601453987346,"user_tz":-210,"elapsed":933,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"2f8da635-ace7-4173-cc06-8bec5ef19c93","colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["definition = 'یک پرنده زیبا'\n","inputs = {'definitions': [definition],\n","          'words':['قناری']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":45,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: قناری</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: یک پرنده زیبا</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspیک&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspپرنده&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspزیبا&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: کلاغ -- بلبل -- گنجشک -- کاکلی -- عقاب -- فاخته -- طاووس -- غوک -- طوطی -- هدهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه قناری را در رتبه 85 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (قناری) را در رتبه 85 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"Ng7ojvk2EPvn","executionInfo":{"status":"ok","timestamp":1601454117580,"user_tz":-210,"elapsed":888,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"44c2f56e-a1f6-465f-d933-66cc9034cb85","colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["definition = 'حل کننده مشکل ویروس کرونا'\n","inputs = {'definitions': [definition],\n","          'words':['واکسن']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":46,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: واکسن</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: حل کننده مشکل ویروس کرونا</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspحل&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspکننده&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspمشکل&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspویروس&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspUNK&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: حاد -- رنگدانه -- محلول -- عفونت -- قرحه -- مسئله -- عفونی -- مبتلا -- میکروب -- هپاتیت</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه واکسن را در رتبه 134 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (واکسن) را در رتبه 134 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"AKu64db5Goe3","executionInfo":{"status":"ok","timestamp":1601454388310,"user_tz":-210,"elapsed":1151,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"a8c9e0eb-9250-4a19-ca03-a6befab9832a","colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["definition = 'تلاش جدی برای رسیدن به هدف'\n","inputs = {'definitions': [definition],\n","          'words':['ممارست']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: ممارست</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: تلاش جدی برای رسیدن به هدف</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspتلاش&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspجدی&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspرسیدن&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspهدف&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: تلاشی -- کوشش -- مبارزه -- ناکامی -- جدیت -- کشمکش -- عزم -- ناکام -- مصمم -- نبرد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه پشتکار را در رتبه 35 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (ممارست) را در رتبه 135 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"UPB8X0KJG22G","executionInfo":{"status":"ok","timestamp":1601454529495,"user_tz":-210,"elapsed":1151,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"d2cb25fa-f175-4a24-84c7-bb4abb83c88e","colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["definition = 'هنر دروغ گفتن به مردم'\n","inputs = {'definitions': [definition],\n","          'words':['ریا']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: ریا</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: هنر دروغ گفتن به مردم</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspهنر&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspدروغ&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspگفتن&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspمردم&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: تهمت -- تزویر -- مضامین -- دورویی -- تحریف -- خرافه -- ریاکاری -- هنرمندانه -- حیله -- مسخرگی</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه تزویر را در رتبه 2 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (ریا) را در رتبه 9351 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"ersCxTJqINb9","executionInfo":{"status":"ok","timestamp":1601455077466,"user_tz":-210,"elapsed":967,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"b1b96565-b20a-4e2e-9b2f-4a32973cbc97","colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["definition = 'دختر کوروش اول'\n","inputs = {'definitions': [definition],\n","          'words':['آتوسا']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":54,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: آتوسا</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: دختر کوروش اول</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspدختر&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspکوروش&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspاول&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: مریم -- مهشید -- ملیکا -- آتوسا -- فاطمه -- ملیحه -- بردیا -- الهه -- میترا -- ناهید</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه آتوسا را در رتبه 4 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (آتوسا) را در رتبه 4 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"Hd2WRw_AImfX","executionInfo":{"status":"ok","timestamp":1601455833058,"user_tz":-210,"elapsed":1833,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"62ba5245-c435-4cd4-d423-c2990dc5f0d4","colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["definition = 'اولین پایتخت رسمی ایران'\n","inputs = {'definitions': [definition],\n","          'words':['همدان']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":55,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: همدان</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: اولین پایتخت رسمی ایران</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspاولین&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspپایتخت&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspرسمی&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspایران&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: قرقیزستان -- فیلیپین -- کویت -- پاکستان -- ونزوئلا -- سنگاپور -- اندونزی -- تایلند -- مراکش -- کامبوج</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه همدان را در رتبه 222 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (همدان) را در رتبه 222 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"P5-9NIX8NLhd","executionInfo":{"status":"ok","timestamp":1601455948216,"user_tz":-210,"elapsed":1197,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"b1b90795-8cd2-41a7-9b1b-a1b9c94e5415","colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["definition = 'آوازش از دور خوش است'\n","inputs = {'definitions': [definition],\n","          'words':['دهل']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":56,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: دهل</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: آوازش از دور خوش است</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspآوازش&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspدور&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspخوش&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: دلنشین -- دلپذیر -- شورانگیز -- لعبت -- آمیغ -- عارفانه -- گشادن -- زیبا -- نغمه -- بلبل</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه دهل را در رتبه 1085 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (دهل) را در رتبه 1085 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"fgM8D6x0Nnyj","executionInfo":{"status":"ok","timestamp":1601456336364,"user_tz":-210,"elapsed":2137,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"c7df25db-9c05-4ed3-ea74-cd580e38e6cd","colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["definition = 'از بدترین کارهایی که یک انسان بخاطر پول ممکن است انجام بدهد'\n","inputs = {'definitions': [definition],\n","          'words':['اختلاس']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":58,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: اختلاس</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: از بدترین کارهایی که یک انسان بخاطر پول ممکن است انجام بدهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspبدترین&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspکارهایی&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspیک&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspانسان&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspبخاطر&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspپول&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspممکن&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspانجام&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspبدهد&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: مخارج -- پولی -- تنگدستی -- خرج -- هزینه -- وجوه -- روپیه -- مبالغ -- احتکار -- صدقه</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اختلاس را در رتبه 14 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (اختلاس) را در رتبه 14 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"LzU1_vvoN4Fk","executionInfo":{"status":"ok","timestamp":1601457026753,"user_tz":-210,"elapsed":1948,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"7fead5c3-2506-4fff-b48b-3b5cc5836f25","colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["definition = 'کسی که از مالیات فرار می‌کند'\n","inputs = {'definitions': [definition],\n","          'words':['UNK']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":61,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: UNK</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: کسی که از مالیات فرار می‌کند</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspکسی&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspمالیات&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspفرار&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspمی‌کند&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: متواری -- ستمکار -- ستدن -- بیچاره -- ترسانیدن -- یاغی -- گروگان -- نوکر -- تنگدستی -- ستیز</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه UNK را در رتبه 20000 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (UNK) را در رتبه 20000 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"3GaQThFVPoIT","executionInfo":{"status":"ok","timestamp":1601457950291,"user_tz":-210,"elapsed":1125,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"425e4de3-11ce-4404-c8d2-5066e82d9520","colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["definition = 'تجمع مسالمت‌آمیز در اعتراض به مسائل سیاسی'\n","inputs = {'definitions': [definition],\n","          'words':['تحصن']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":64,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: تحصن</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: تجمع مسالمت‌آمیز در اعتراض به مسائل سیاسی</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspتجمع&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspمسالمت‌آمیز&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspاعتراض&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspمسائل&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspسیاسی&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: تحصن -- اعتراضات -- سرکوب -- شورش -- ضدیت -- حکومت -- سرنگونی -- مخالفت -- تعرض -- کودتا</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه تحصن را در رتبه 1 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (تحصن) را در رتبه 1 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"CfZeYEcySUbS"},"source":[""],"execution_count":null,"outputs":[]}]}