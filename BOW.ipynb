{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of BOW.ipynb","provenance":[{"file_id":"16LC16BtCxDitLEXzZlj5xDswL-v6NFI3","timestamp":1600883903974}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"nFN0j4666TpJ"},"source":["# Persian Reverse Dictionary\n","\n","This notebook provides instructions needed to repeat the experiments we have done to train a [Persian reverse dictionary model](https://arxiv.org/abs/2105.00309). To start, please modify the following variables in the \"configuration\" section based on your desire:\n","\n","- `acc_eval_sample_size`: The size of the sample used to measure the accuracy of the model\n","\n","- `add_context`: You can set this variable to True if you want the model to learn from the definitions of many words, despite not recognizing them as valid outputs. For instance, If the word \"آب آشنا\" is ranked 20000th, and you have set the variable \"num_head_words\" to 10000, the model won't consider the definitions of this word as its training samples. However, if you set the variable \"add_context\" to True, it will.\n","\n","- `augment_gloss`: Set it to True if you want the model to make new training samples from the existing ones by replacing words in the definitions by their synonyms. For instance, if the word \"آب آشنا\" means \"کسی که شناوری بداند\" and the two words \"کسی\" and \"شخصی\" are considered as synonyms, the model will generate a new definition for the word \"آب آشنا\" like this: \"شخصی که شناوری بداند\".\n","\n","- `augment_head`: Set it to True if you want the model to make new training samples from the existing ones by considering the definition of a word as the definition of its synonyms. For instance, if the words \"آب آشنا\" and \"شناگر\" are considered as synonyms and the word \"آب آشنا\" means \"کسی که شناوری بداند\", the model will consider this definition also as the definition of the word \"شناگر\".\n","\n","- `batch_size`: The size of the batches of data used to train the model\n","\n","- `context_max_rank`: The value of this variable indicates the maximum rank of the words for which their definitions is going to be used as the model's training samples. Only works if the variable \"add_context\" is set to True.\n","\n","- `dense_output_size`: The size of the output vectors given by the Dense layers\n","\n","- `data_sources`: The sources to be used during the training.\n","\n","- `fixed_embeddings`: Set this variable to True if you want the model to leave the embeddings of the input words unchanged during the training phase. Otherwise, they will be trained by the samples.\n","\n","- `gfake_per_sample`: A dictionary in which keys indicate how many words should be replaced when the \"augment_gloss\" is set to True, and the corresponding values indicate the maximum number of new samples that should be generated by replacing them. For instance, 3:5 means that 3 words in the definition should be replaced by their synonyms, and at most 5 unique new samples should be generated using this method.\n","\n","- `gloss_max_rank`: The maximum rank of the words recognized by the model in the input sequence. For instance, if this variable is set to 5000 and the word \"آب آشنا\" is ranked 6000th by frequency, the model will replace \"آب آشنا\" with \"UNK\".\n","\n","- `input_emb_size`: The size of the input embeddings that will be learned by the model. Please set this variable to 300 if you are using the original \"FastText\" embeddings used by us.\n","\n","- `input_vector_model`: The model (set of embeddings) to be used when training the model. It should exist in the keys of the \"vector_model_dir\".\n","\n","- `learning_rate`: The learning rate to start from. It will be changed according to the AdaDelta optimization algorithm.\n","\n","- `loss_function`: Can be 'cosine' or 'ranking'. Use 'cosine' if you want the model to improve the cosine similarity between its output and the gold one. Otherwise, use 'ranking' to calculate the rank loss (also called the triplet loss in some papers).\n","\n","- `lstm_output_size`: The size of the vectors that are given by the LSTM layers\n","\n","- `make_hfake_per_sample`: The number of fake samples to generate when the variable \"augment_head\" is set to True\n","\n","- `margin`: The margin to be used for the rank loss.\n","\n","- `max_seq_len`: The maximum length of a sequence of words given to the model as its raw input\n","\n","- `model_architecture`: This variable's value can be set to \"bow\", \"lstm\", \"bilstm\", \"lstmatt\", \"bilstmatt\", and \"bilstmmultiatt\". The mentioned values correspond to using \"bag of words\", \"LSTM\", \"Bidirectional LSTM (BiLSTM)\", \"LSTM with Attention\", \"BiLSTM with Attention\", \"BiLSTM with MultiHeadAttention\" models, relatively. The last model was not reported in the paper, due to its poor results despite huge amount of time one should spend on training it.\n","\n","- `mos_eval_sample_size`: The size of the sample used to measure the Mean Opinion Score (MOS)\n","\n","- `normalize_vectors`: Set this to True if you want the embeddings of the input vectors to be normalized\n","\n","- `num_epochs`: The number of epochs to train the model\n","\n","- `num_head_words`: The number of words you want the model to recognize. The model automatically selects the top N words based on a ranking. \n","\n","- `num_heads`: The number of attention heads to be used if the model uses multihead attention.\n","\n","- `output_emb_size`: The size of the output vector that the whole model generates\n","\n","- `output_vector_model`: The model (set of embeddings) to be used when training the model. It should exist in the keys of the \"vector_model_dir\".\n","\n","- `path_prefix`: Set the value of this variable to the path in which all of the project files are stored. Particularly, it should be the path containing this notebook, and the python files that are imported (including \"experiment_v8.py\", \"globals.py\", and \"attention.py\")\n","\n","- `pretrained_target`: Set it to True if you want the model to use FastText embeddings as the gold output vectors. Otherwise, the model will try to learn them as well (not recommended!).\n","\n","- `random_seed`: This variable helps repeating the experiment under the same conditions. There is no need to change this.\n","\n","- `result_topn`: The maximum number of words the model should output, given a sequence of words (Used by the \"query\" function)\n","\n","- `save_tools`: Set it to True if you want the model to remember the IDs assigned to each word (used to query the model after it has been trained)\n","\n","- `save_train_test_samples`: Set it to True if you want the model to save the training samples to be used after the training\n","\n","- `vector_model_dir`: The path to a pickle file containing a python dictionary consisting of words as keys, and their embeddings (vectors) as the corresponding values. Note that this variable contains a key for each vector list. For instance, if you have gathered that pickle file using fasttext embeddings, you need to specify it as a key named 'fasttext' (or whatever you like). The value corresponding to this key should be the actual path of the embeddings"]},{"cell_type":"code","metadata":{"id":"OVTrklR16fhU"},"source":["pip install tensorflow==2.3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZRYVm9y1eAC"},"source":["from google.colab import drive\n","import warnings"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FxZH0CrXWAlN"},"source":["**Authentication**"]},{"cell_type":"code","metadata":{"id":"y7fiSgckBIpF","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1601453223416,"user_tz":-210,"elapsed":4157,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"1382afa1-afdf-4400-fc5e-872c1884d88f"},"source":["drive.mount('/gdrive', force_remount=True)\n","%cd /gdrive/My Drive/Persian-Reverse-Dictionary/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n","/gdrive/My Drive/Persian-Reverse-Dictionary\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bMzxZkFjWHca"},"source":["**Importing the necessary libraries**"]},{"cell_type":"code","metadata":{"id":"wQ224cPaBO8L"},"source":["from experiment_v8 import *\n","from globals import *\n","from attention import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_lup1Ja_WcPF"},"source":["**Configuration**"]},{"cell_type":"code","metadata":{"id":"CLytgx5NCDvq"},"source":["path_prefix = '/path/to/the/folder/Persian-Reverse-Dictionary/'\n","np.set_printoptions(suppress=True)\n","experiment_phase = 'training' # phase: training/querying/continue\n","warnings.filterwarnings(\"ignore\")\n","random_seed = 1234\n","tf.random.set_seed(random_seed)\n","seed(random_seed)\n","shared_files = read_csv_as_dict('shared_files.csv')\n","globals().update(shared_files)\n","model_architecture = 'lstmatt' # choose 'bow', 'lstm', 'lstmatt', 'bilstm', 'bilstmatt', or'bilstmmultiadd'\n","project_path = path_prefix+'models/'+model_architecture+'/'\n","num_head_words = 20000\n","folder_path = str(num_head_words)+'/'\n","gloss_max_rank = 100000\n","input_emb_size = 300\n","lstm_output_size = 300\n","dense_output_size = 300\n","output_emb_size = 300\n","pretrained_input = True\n","fixed_embeddings = False\n","pretrained_target = True\n","normalize_vectors = False\n","add_context = False\n","augment_head = False\n","augment_gloss = False\n","save_train_test_samples = False\n","save_tools = False\n","max_hfake_per_sample = 5\n","gfake_per_sample = {\n","    3:5,\n","    4:5\n","}\n","context_max_rank = 100000\n","max_seq_len = 20\n","learning_rate = 1.0\n","margin = 1.0\n","num_epochs = 1 # 50\n","batch_size = 16\n","mos_eval_sample_size = 40\n","result_topn = 10\n","acc_eval_sample_Size = 500\n","input_vector_model = 'fasttext'\n","output_vector_model = 'fasttext'\n","data_sources = {'wikipedia','amid','dehkhoda-vy','moeen-vy','farsnet'}\n","vector_model_dir = {'fasttext':fasttext_dict_dir}\n","num_heads = 8\n","d_model = 300\n","loss_function = 'cosine' # choose 'cosine', or 'ranking'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WSYzfzFTHopD"},"source":["import os\n","try:\n","    os.stat(project_path+folder_path)\n","except:\n","    os.mkdir(project_path+folder_path)\n","if experiment_phase == 'training':\n","    log_dir = project_path+folder_path+'report.dat'\n","    log_obj = {\n","        'gloss':str(gloss_max_rank),\n","        'head':str(num_head_words),\n","        'input_emb':str(input_emb_size),\n","        'output_emb':str(output_emb_size),\n","        'dense_output':str(dense_output_size),\n","        'lstm_output':str(lstm_output_size),\n","        'pretrained_input':input_vector_model if pretrained_input else 'no',\n","        'pretrained_target':output_vector_model if pretrained_target else 'no',\n","        'fixed_embeddings':'yes' if fixed_embeddings else 'no',\n","        'normalize_vectors':'yes' if normalize_vectors else 'no',\n","        'context':str(context_max_rank) if add_context else 'no',\n","        'augment_head':str(max_hfake_per_sample) if augment_head else 'no',\n","        'augment_gloss':str(gfake_per_sample) if augment_gloss else 'no',\n","        'margin':str(margin),\n","        'seq_len':str(max_seq_len),\n","        'learning_rate':str(learning_rate),\n","        'batch_size':str(batch_size),\n","        'data_sources':\" \".join(data_sources),\n","        'model_architecture':model_architecture,\n","        'loss_function':loss_function,\n","    }\n","    init_log(log_obj, log_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Dhnb4tXWtDP"},"source":["**Loading**<br/>\n","* Data \n","* Stopwords\n","* Normalizer\n","* Ranking of words by frequency\n","* Synonyms Set\n","* POS Tags\n","* Vector Model(s)"]},{"cell_type":"code","metadata":{"id":"4Fc3W5BaNM4-"},"source":["word_tags = defaultdict(set)\n","ranking = read_pickle(ranking_dir)\n","all_words = set(ranking)\n","stopwords = read_lines(stopwords_dir)\n","normalizer = read_json(normalizer_dir)\n","synonyms = read_pickle(synonyms_dir)\n","if experiment_phase in {'training','continue'}:\n","    data = read_json(data_dir)\n","input_vec_model = read_pickle(vector_model_dir[input_vector_model])\n","output_vec_model = read_pickle(vector_model_dir[input_vector_model])\n","tools = {'normalizer':normalizer,'stopwords':stopwords,'synonyms':synonyms,'word_tags':word_tags, 'output_emb_size':output_emb_size}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HV3uI4xT7NYd"},"source":["tmp_data = []\n","for item in data:\n","    for phrase in item['phrases']:\n","        new_item = {'word':item['word'], 'dic':item['source'],'meaning':phrase}\n","        tmp_data.append(new_item)\n","data_len = len(tmp_data)\n","for idx, item in enumerate(tmp_data):\n","    if idx < int(data_len*0.8):\n","        tmp_data[idx]['phase'] = 'train'\n","    elif idx >= int(data_len*0.8) and idx < int(data_len*0.9):\n","        tmp_data[idx]['phase'] = 'test'\n","    else:\n","        tmp_data[idx]['phase'] = 'dev'\n","data = tmp_data[:]\n","del tmp_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pB24i9DjCopu"},"source":["items = []\n","sources_counter = Counter()\n","word_rank = {word:idx+1 for idx, word in enumerate(ranking)}\n","if experiment_phase in {'training','continue'}:\n","    for item in data:\n","        sources_counter[item['dic']] += 1\n","        new_item = {'word':item['word'],\n","                    'rank':word_rank[item['word']]+1 if item['word'] in all_words else len(ranking)+1,\n","                    'original_definition':item['meaning'],\n","                    'preprocessed_definition':item['meaning'],\n","                    'active':True,\n","                    'array':None,\n","                    'context':False,\n","                    'type':'main',\n","                    'source':item['dic'],\n","                    'phase':item['phase']\n","                    }\n","        items.append(new_item)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5VzY8W9_XFQh"},"source":["**Preparing the Train, Test and Dev Sets**"]},{"cell_type":"code","metadata":{"id":"B01R_HZH8WKA"},"source":["if experiment_phase in {'training','continue'}:\n","    ## fixing the data sources\n","    for item in items:\n","        if item['source'] not in data_sources:\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    ## removing html tags\n","    for item in items:\n","        item['word'] = remove_html_tags(item['word'])\n","        item['original_definition'] = remove_html_tags(item['original_definition'])\n","        item['preprocessed_definition'] = remove_html_tags(item['preprocessed_definition'])\n","    ## deactivating samples without any Persian information\n","    for item in items:\n","        if not (has_any_persian(item['word']) and has_any_persian(item['original_definition'])):\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'with Persian information - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","    ## normalizing characters\n","    for item in items:\n","        item['preprocessed_definition'] = normalize_characters(item['preprocessed_definition'], normalizer)\n","        item['word'] = normalize_characters(item['word'], normalizer)\n","    ## correcting whitespaces\n","    for item in items:\n","        item['word'] = correct_whitespaces(item['word'])\n","        item['preprocessed_definition'] = correct_whitespaces(item['preprocessed_definition'])\n","    ## tokenization\n","    for item in items:\n","        item['preprocessed_definition'] = tokenize(item['preprocessed_definition'])\n","    ## removing self-definition(s)\n","    for item in items:\n","        item['preprocessed_definition'] = remove_self_definition(item['preprocessed_definition'], item['word'])\n","    ## removing stopwords\n","    for item in items:\n","        item['preprocessed_definition'] = remove_stopwords(item['preprocessed_definition'], stopwords)\n","    for item in items:\n","        if item['word'] in stopwords:\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'Stopwords removed - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","    ## deactivating unlearnables\n","    for item in items:\n","        if not has_vector(item['word'], output_vec_model):\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'Unlearnables Removed - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","    ## deactivating short words\n","    for item in items:\n","        if len(item['word']) < 3:\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'Short words removed - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","    ## deactivating short definitions\n","    for item in items:\n","        if len(item['preprocessed_definition']) == 1 and len(item['preprocessed_definition'][0]) < 3:\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'Short definitions removed - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","    ## deactivating meaningless words\n","    for item in items:\n","        if len(item['preprocessed_definition']) == 0:\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'Meaningless words removed - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","    ## normalizing heads by ranking (frequency)\n","    active_words = set([item['word'] for item in items if item['active']==True])\n","    temp_ranking = [word for word in ranking if word in active_words]\n","    frequent_words = set([word for word in temp_ranking][:num_head_words])\n","    context_words = set([word for word in temp_ranking][:context_max_rank])\n","    for item in items:\n","        if item['word'] not in frequent_words:\n","            if add_context == True:\n","                if item['word'] in context_words:\n","                    item['phase'] = 'train'\n","                    item['context'] = True\n","                else:\n","                    item['active'] = False\n","            else:\n","                item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    main_words = set([item['word'] for item in items if item['context']==False])\n","    log_str = 'Normalized heads by frequency - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words)) +' ('+str(len(main_words))+' Main Words)'\n","    write_line_to_file(log_str, log_dir)\n","    ## generating the comparison matrix\n","    h2id = {}\n","    id2h = {}\n","    for i, h in enumerate(frequent_words):\n","        h2id[h] = i\n","        id2h[i] = h\n","    comparison_matrix = zeros((num_head_words, output_emb_size))\n","    if pretrained_target == True:\n","        for item in items:\n","            if item['phase'] == 'train' and item['word'] in frequent_words:\n","                comparison_matrix[h2id[item['word']]] = output_vec_model[item['word']]\n","    ## normalizing the tokens based on their frequency\n","    tokens_lst = []\n","    for item in items:\n","        if item['phase'] == 'train':\n","            tokens_lst.extend(item['preprocessed_definition'])\n","    frequent_tokens = most_frequent(tokens_lst, gloss_max_rank)\n","    for item in items:\n","        item['preprocessed_definition'] = [token if token in frequent_tokens else 'UNK' for token in item['preprocessed_definition']]\n","        if 'UNK' in item['preprocessed_definition']:\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    main_words = set([item['word'] for item in items if item['context']==False])\n","    log_str = 'Normalized tokens by frequency - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words)) +' ('+str(len(main_words))+' Main Words)'\n","    write_line_to_file(log_str, log_dir)\n","    ## encoding the tokens\n","    t2id = {}\n","    id2t = {}\n","    t2id['PAD'] = 0\n","    t2id['UNK'] = 1\n","    id2t[0] = 'PAD'\n","    id2t[1] = 'UNK' \n","    for i, t in enumerate(frequent_tokens, start=2):\n","        t2id[t] = i\n","        id2t[i] = t\n","    ## generating the embedding matrix\n","    embedding_matrix = zeros((len(frequent_tokens)+2, input_emb_size))\n","    if pretrained_input == True:\n","        for token in frequent_tokens:\n","            try:\n","                embedding_matrix[t2id[token]] = input_vec_model[token]\n","                if normalize_vectors == True and LA.norm(embedding_matrix[t2id[token]])>0.0:\n","                    embedding_matrix[t2id[token]] = embedding_matrix[t2id[token]]/LA.norm(embedding_matrix[t2id[token]])\n","            except:\n","                continue\n","\n","    ## fixing the length of sequences\n","    for item in items:\n","        tokens = item['preprocessed_definition']\n","        item['preprocessed_definition'] = tokens[:max_seq_len] if len(tokens)>=max_seq_len else tokens+['PAD' for j in range(max_seq_len-len(tokens))]\n","    ## removing the words with definitions that are only consisted of 'PAD' and 'UNK'\n","    for item in items:\n","        tokens = item['preprocessed_definition']\n","        tokens = set(tokens)\n","        if tokens.issubset({'PAD','UNK'}):\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'Infoless definitions (UNK and PAD) removed - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","\n","    ## removing duplicates (samples with equal word and definition)\n","    # from the training data\n","    sample_count = Counter()\n","    for item in items:\n","        if item['phase'] == 'train':\n","            word = item['word']\n","            tokens = item['preprocessed_definition']\n","            sample_key = word+'-'+\"-\".join(tokens)\n","            sample_count[sample_key] += 1\n","            if sample_count[sample_key] > 1:\n","                item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    main_words = set([item['word'] for item in items if item['context']==False])\n","    log_str = 'duplicates removed from training data - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))+' ('+str(len(main_words))+' Main Words)'\n","    write_line_to_file(log_str, log_dir)\n","    # from the testing and development data\n","    for item in items:\n","        if item['phase'] != 'train':\n","            word = item['word']\n","            tokens = item['preprocessed_definition']\n","            sample_key = word+'-'+\"-\".join(tokens)\n","            sample_count[sample_key] += 1\n","            if sample_count[sample_key] > 1:\n","                item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    main_words = set([item['word'] for item in items if item['context']==False])\n","    log_str = 'duplicates removed from unseen data - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))+' ('+str(len(main_words))+' Main Words)'\n","    write_line_to_file(log_str, log_dir)\n","    ## generating the data\n","    for item in items:\n","        tokens = item['preprocessed_definition']\n","        nparray = [t2id[token] for token in tokens]\n","        nparray = array(nparray)\n","        item['array'] = nparray\n","    train = [item for item in items if item['phase'] == 'train']\n","    test = [item for item in items if item['phase'] == 'test']\n","    dev = [item for item in items if item['phase'] == 'dev']\n","    log_str = 'After Preprocessing: '+str(len(train))+' training samples, '+str(len(test))+' testing samples and '+str(len(dev))+' samples for development'\n","    write_line_to_file(log_str, log_dir)\n","    tools_part2 = {'t2id':t2id,'id2t':id2t,'h2id':h2id,'id2h':id2h,'embedding_matrix':embedding_matrix,'comparison_matrix':comparison_matrix}\n","    tools.update(tools_part2)\n","    train_sources_counter = Counter()\n","    for item in train:\n","        train_sources_counter[item['source']] += 1\n","    test_sources_counter = Counter()\n","    for item in test:\n","        test_sources_counter[item['source']] += 1\n","    dev_sources_counter = Counter()\n","    for item in dev:\n","        dev_sources_counter[item['source']] += 1\n","    write_line_to_file('Training Data: '+str(sum(train_sources_counter.values())), log_dir)\n","    write_line_to_file(str(train_sources_counter), log_dir)\n","    write_line_to_file('Development Data: '+str(sum(dev_sources_counter.values())), log_dir)\n","    write_line_to_file(str(dev_sources_counter), log_dir)\n","    write_line_to_file('Testing Data: '+str(sum(test_sources_counter.values())), log_dir)\n","    write_line_to_file(str(test_sources_counter), log_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XOsqxIYlGMZ7"},"source":["if experiment_phase == 'training':\n","    words = list(set(list(h2id.keys())))\n","    with open(project_path+folder_path+'words'+str(num_head_words)+'.pkl','wb') as file:\n","        pickle.dump(words, file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRtgkwzAWtGn"},"source":["if experiment_phase in {'training','continue'}:\n","    from copy import deepcopy\n","    # Making a counter to see if a sample already exists\n","    item_count = Counter()\n","    for item in items:\n","        word = item['word']\n","        tokens = item['preprocessed_definition']\n","        definition = \" \".join(tokens)\n","        key = word+'-'+definition\n","        item_count[key] += 1\n","\n","    # Augmenting (head words)\n","\n","    head_augment = []\n","    all_heads = set([item['word'] for item in items if item['phase']=='train'])\n","    replaceable_heads = set([word for word in all_heads if word in synonyms])\n","    if augment_head == True:\n","        for item in train:\n","            if len(train) % 100000 == 0:\n","                print(len(train))\n","            word = item['word']\n","            tokens = item['preprocessed_definition']\n","            definition = \" \".join(tokens)\n","            syn_words = synonyms[word]\n","            syn_words = set([w for w in syn_words if w in output_vec_model and w != word])\n","            if len(syn_words) == 0:\n","                continue\n","            if len(syn_words) >= max_hfake_per_sample:\n","                syn_candidates = sample(syn_words, max_hfake_per_sample)\n","            else:\n","                syn_candidates = list(syn_words)\n","            for candidate in syn_candidates:\n","                new_item = item.copy()\n","                new_item['word'] = candidate\n","                new_item['type'] = 'augment'\n","                key = candidate+'-'+definition\n","                if item_count[key] == 0:\n","                    head_augment.append(new_item)\n","                    item_count[key] += 1\n","\n","    # Augmenting (gloss words)\n","\n","    gloss_augment = []\n","\n","\n","    usable_synonyms = defaultdict()\n","    for (word, synset) in synonyms.items():\n","        temp_synset = set([syn for syn in synset if syn != word and syn in t2id])\n","        if len(temp_synset) > 0:\n","            usable_synonyms[word] = temp_synset\n","\n","    gloss_augment = []\n","\n","    if augment_gloss == True:\n","        for item in train:\n","            \n","            if item['type'] != 'main':\n","                continue\n","            word = item['word']\n","            tokens = item['preprocessed_definition']\n","            definition = \" \".join(tokens)\n","            rep_tokens = set([token for token in tokens if token in usable_synonyms and token in input_vec_model])\n","            for num_tokens_to_replace in gfake_per_sample.keys():\n","                if len(rep_tokens) < num_tokens_to_replace:\n","                    continue\n","                remaining_samples = gfake_per_sample[num_tokens_to_replace]\n","                remaining_tries = 50\n","                while remaining_samples > 0 and remaining_tries > 0:\n","                    chosen_to_be_changed = set(sample(rep_tokens, num_tokens_to_replace))\n","                    what_to_replace_with = defaultdict()\n","                    for token in chosen_to_be_changed:\n","                        candidates = usable_synonyms[token]\n","                        what_to_replace_with[token] = sample(candidates, 1)[0]\n","                    new_item = deepcopy(item)\n","                    new_item['preprocessed_definition'] = [what_to_replace_with[t] if t in chosen_to_be_changed else t for t in new_item['preprocessed_definition']]\n","                    new_item['array'] = np.array([t2id[t] for t in new_item['preprocessed_definition']])\n","                    new_item_key = new_item['word'] + '-' + \" \".join(new_item['preprocessed_definition'])\n","                    remaining_tries -= 1\n","                    if item_count[new_item_key] == 0:\n","                        gloss_augment.append(new_item)\n","                        item_count[new_item_key] += 1\n","                        remaining_samples -= 1\n","\n","\n","    train.extend(head_augment)\n","    del head_augment\n","    log_str = 'finished augmentation (head) - '+str(len(train))+' samples'\n","    write_line_to_file(log_str, log_dir)\n","    train.extend(gloss_augment)\n","    del gloss_augment\n","    log_str = 'finished augmentation (gloss) - '+str(len(train))+' samples'\n","    write_line_to_file(log_str, log_dir)\n","    del items"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JDRxe3_DeGMn"},"source":["if experiment_phase == 'training' and save_train_test_samples == True:\n","    train_data = defaultdict(list)\n","    test_data = defaultdict(list)\n","    train_original_data = defaultdict(list)\n","    test_original_data = defaultdict(list)\n","    for item in train:\n","        word = item['word']\n","        original_definition = \" \".join(item['original_definition'].split()[:max_seq_len])\n","        definition = \" \".join([token for token in item['preprocessed_definition'] if not token == 'PAD'])\n","        train_data[word].append(definition)\n","        train_original_data[word].append(original_definition)\n","    for item in test:\n","        word = item['word']\n","        original_definition = \" \".join(item['original_definition'].split()[:max_seq_len])\n","        definition = \" \".join([token for token in item['preprocessed_definition'] if not token == 'PAD'])\n","        test_data[word].append(definition)\n","        test_original_data[word].append(original_definition)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"exyXpM20Sh7Y"},"source":["if experiment_phase in {'training','continue'}:\n","    x_train = array([item['array'] for item in train])\n","    y_train = array([comparison_matrix[h2id[item['word']]] if item['word'] in h2id else output_vec_model[item['word']] for item in train])\n","    x_dev = array([item['array'] for item in dev])\n","    y_dev = array([comparison_matrix[h2id[item['word']]] if item['word'] in h2id else output_vec_model[item['word']] for item in dev])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qTHzcwVEXQ3k"},"source":["**Saving the necessary variables**"]},{"cell_type":"code","metadata":{"id":"CAMMkkIVXR6V"},"source":["if experiment_phase == 'training':\n","    if save_tools == True:\n","        with open(project_path+folder_path+'tools.pkl','wb') as file:\n","            pickle.dump(tools, file)\n","    if save_train_test_samples == True:\n","        train_test_data = (train_data, train_original_data, test_data, test_original_data)\n","        with open(project_path+folder_path+'train_test_data.pkl','wb') as file:\n","            pickle.dump(train_test_data, file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mx7KX9xvKhPi"},"source":["if experiment_phase == 'training':\n","    emb_matrix_shape = embedding_matrix.shape\n","    additional_tools = {'comparison_matrix':comparison_matrix, 't2id':t2id, 'h2id':h2id, 'id2h':id2h}\n","    query_tools = (emb_matrix_shape, additional_tools)\n","    with open(project_path+folder_path+'query_tools.pkl','wb') as file:\n","        pickle.dump(query_tools, file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F_gBuN56Ivew"},"source":["**Loading Necessary Tools for Querying**"]},{"cell_type":"code","metadata":{"id":"bWelxKZ2KJeJ"},"source":["if experiment_phase == 'querying':\n","    emb_matrix_shape , additional_tools = read_pickle(project_path+folder_path+'query_tools.pkl')\n","    tools.update(additional_tools)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WsWo-X61Xiku"},"source":["**Making a Model and Training it**"]},{"cell_type":"markdown","metadata":{"id":"16aucIuAj_Gy"},"source":["*Bag of Words*"]},{"cell_type":"code","metadata":{"id":"pRZhlJhpiipI"},"source":["if model_architecture == 'bow':\n","    sequence_input = Input(shape=(max_seq_len,), dtype='int32', name='input')\n","    embedding_layer = Embedding(emb_matrix_shape[0],\n","                                input_emb_size,\n","                                weights=[embedding_matrix] if 'embedding_matrix' in globals() else [np.zeros(emb_matrix_shape)],\n","                                input_length=max_seq_len,\n","                                trainable=not fixed_embeddings,\n","                                mask_zero=True, name='embedding')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    mean_vector = Lambda(lambda x: mean(x, axis=1), name='lambda')(embedded_sequences)\n","    mean_vector = Flatten(name='flatten')(mean_vector)\n","    output = Dense(dense_output_size, activation='tanh', name='dense')(mean_vector)\n","    adadelta = optimizers.Adadelta(lr=learning_rate)\n","    model = Model(inputs=sequence_input, outputs=output)\n","    if loss_function == 'cosine':\n","        model.compile(loss=cosine_loss,\n","                    optimizer=adadelta,metrics=[cosine_sim])\n","    elif loss_function == 'ranking':\n","        model.compile(loss=rank_loss(margin),\n","                    optimizer=adadelta,metrics=[cosine_sim])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2o1UORkqk6Ed"},"source":["*LSTM (without attention)*"]},{"cell_type":"code","metadata":{"id":"hGT_QZftoYWw"},"source":["if model_architecture == 'lstm':\n","    sequence_input = Input(shape=(max_seq_len,), dtype='int32', name='input')\n","    embedding_layer = Embedding(emb_matrix_shape[0],\n","                                input_emb_size,\n","                                weights=[embedding_matrix]  if 'embedding_matrix' in globals() else [np.zeros(emb_matrix_shape)],\n","                                input_length=max_seq_len,\n","                                trainable=not fixed_embeddings,\n","                                mask_zero=True, name='embedding')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    lstm = LSTM(lstm_output_size, return_sequences=False, name='lstm')(embedded_sequences)\n","    output = Dense(dense_output_size, activation='tanh', name='dense')(lstm)\n","    adadelta = optimizers.Adadelta(lr=learning_rate)\n","    model = Model(inputs=sequence_input, outputs=output)\n","    if loss_function == 'cosine':\n","        model.compile(loss=cosine_loss,\n","                    optimizer=adadelta,metrics=[cosine_sim])\n","    elif loss_function == 'ranking':\n","        model.compile(loss=rank_loss(margin),\n","                    optimizer=adadelta,metrics=[cosine_sim])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FkWxjSIZlwAA"},"source":["*BiLSTM (without attention)*"]},{"cell_type":"code","metadata":{"id":"6SWCnEdXlyU0"},"source":["if model_architecture == 'bilstm':\n","    sequence_input = Input(shape=(max_seq_len,), dtype='int32', name='input')\n","    embedding_layer = Embedding(emb_matrix_shape[0],\n","                                input_emb_size,\n","                                weights=[embedding_matrix] if 'embedding_matrix' in globals() else [np.zeros(emb_matrix_shape)],\n","                                input_length=max_seq_len,\n","                                trainable=not fixed_embeddings,\n","                                mask_zero=True, name='embedding')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    lstm = Bidirectional(LSTM(lstm_output_size, return_sequences=False, name='lstm'))(embedded_sequences)\n","    output = Dense(dense_output_size, activation='tanh', name='dense')(lstm)\n","    adadelta = optimizers.Adadelta(lr=learning_rate)\n","    model = Model(inputs=sequence_input, outputs=output)\n","    if loss_function == 'cosine':\n","        model.compile(loss=cosine_loss,\n","                    optimizer=adadelta,metrics=[cosine_sim])\n","    elif loss_function == 'ranking':\n","        model.compile(loss=rank_loss(margin),\n","                    optimizer=adadelta,metrics=[cosine_sim])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CBxsYqtk1z6J"},"source":["*LSTM (with attention)*"]},{"cell_type":"code","metadata":{"id":"N_kTQtT2Wkmk"},"source":["if model_architecture == 'lstmatt':\n","    sequence_input = Input(shape=(max_seq_len,), dtype='int32', name='input')\n","    embedding_layer = Embedding(emb_matrix_shape[0],\n","                                input_emb_size,\n","                                weights=[embedding_matrix] if 'embedding_matrix' in globals() else [np.zeros(emb_matrix_shape)],\n","                                input_length=max_seq_len,\n","                                trainable=not fixed_embeddings,\n","                                mask_zero=True, name='embedding')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    lstm, state_h, state_c = LSTM(lstm_output_size, return_sequences=True, return_state=True, name='lstm')(embedded_sequences)\n","    state_h = Lambda(lambda x: tf.expand_dims(x, 1), name='expand_dims')(state_h)\n","    state_h = Dense(300, name='d1')(state_h)\n","    lstm = Dense(300, name='d2')(lstm)\n","    weights, context_vector = AdditiveAttention(name='attention')([state_h, lstm])\n","    weights = Lambda(lambda x: tf.squeeze(x), name='squeezed_weights')(weights)\n","    output = Dense(dense_output_size, activation='tanh', name='dense')(context_vector)\n","    output = Lambda(lambda x: tf.squeeze(x), name='squeezed_output')(output)\n","    adadelta = optimizers.Adadelta(lr=learning_rate)\n","    model = Model(inputs=sequence_input, outputs=output)\n","    attention_model = Model(inputs=sequence_input, outputs=weights)\n","    if loss_function == 'cosine':\n","        model.compile(loss=cosine_loss,\n","                    optimizer=adadelta,metrics=[cosine_sim])\n","    elif loss_function == 'ranking':\n","        model.compile(loss=rank_loss(margin),\n","                    optimizer=adadelta,metrics=[cosine_sim])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bqiOp6yVl_Fe"},"source":["*BiLSTM (with attention)*"]},{"cell_type":"code","metadata":{"id":"z_CcCLEK0r8O"},"source":["if model_architecture == 'bilstmatt':\n","    sequence_input = Input(shape=(max_seq_len,), dtype='int32', name='input')\n","    embedding_layer = Embedding(emb_matrix_shape[0],\n","                                input_emb_size,\n","                                weights=[embedding_matrix] if 'embedding_matrix' in globals() else [np.zeros(emb_matrix_shape)],\n","                                input_length=max_seq_len,\n","                                trainable=not fixed_embeddings,\n","                                mask_zero=True, name='embedding')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(lstm_output_size, return_sequences=True, return_state=True, name='lstm'))(embedded_sequences)\n","    state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n","    state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n","    state_h = tf.expand_dims(state_h, 1)\n","    state_h = Dense(300, name='d1')(state_h)\n","    lstm = Dense(300, name='d2')(lstm)\n","    weights, context_vector = AdditiveAttention()([state_h, lstm])\n","    weights = tf.squeeze(weights)\n","    output = Dense(dense_output_size, activation='tanh', name='dense')(context_vector)\n","    output = tf.squeeze(output)\n","    adadelta = optimizers.Adadelta(lr=learning_rate)\n","    model = Model(inputs=sequence_input, outputs=output)\n","    attention_model = Model(inputs=sequence_input, outputs=weights)\n","    if loss_function == 'cosine':\n","        model.compile(loss=cosine_loss,\n","                    optimizer=adadelta,metrics=[cosine_sim])\n","    elif loss_function == 'ranking':\n","        model.compile(loss=rank_loss(margin),\n","                    optimizer=adadelta,metrics=[cosine_sim])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VUXgFCzbhTo"},"source":["# lstm: [2, 20, 600]\n","# state_h: [2, 600]\n","# new_lstm (after Dense(300)):  [2, 20, 300]\n","# new_state_h (after expand_dims): [2, 1, 600]\n","# new_state_h (after Dense(300)): [2, 1, 300]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"68oE-F-AQFw3"},"source":["*Multi-head Attention*"]},{"cell_type":"code","metadata":{"id":"YFDwQjGbSvs3"},"source":["if model_architecture == 'bilstmmultiatt':\n","    sequence_input = Input(shape=(max_seq_len,), dtype='int32', name='input')\n","    embedding_layer = Embedding(emb_matrix_shape[0],\n","                                input_emb_size,\n","                                weights=[embedding_matrix] if 'embedding_matrix' in globals() else [np.zeros(emb_matrix_shape)],\n","                                input_length=max_seq_len,\n","                                trainable=not fixed_embeddings,\n","                                mask_zero=True, name='embedding')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(lstm_output_size, return_sequences=True, return_state=True, name='lstm'))(embedded_sequences)\n","    state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n","    state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n","    state_h = tf.expand_dims(state_h, 1)\n","    context_vector = MultiheadAttention(num_heads, d_model)([state_h, lstm, lstm])\n","    output = Dense(dense_output_size, activation='tanh', name='dense')(context_vector)\n","    output = tf.squeeze(output)\n","    adadelta = optimizers.Adadelta(lr=learning_rate)\n","    model = Model(inputs=sequence_input, outputs=output)\n","    if loss_function == 'cosine':\n","        model.compile(loss=cosine_loss,\n","                    optimizer=adadelta,metrics=[cosine_sim])\n","    elif loss_function == 'ranking':\n","        model.compile(loss=rank_loss(margin),\n","                    optimizer=adadelta,metrics=[cosine_sim])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6XAA9tjEwT09"},"source":["if experiment_phase == 'training':\n","    with open(log_dir,'a+',encoding='utf-8') as file:\n","        model.summary(print_fn=lambda line: file.write(line + '\\n'))\n","    write_line_to_file(str(model.loss), log_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VtLV5GUxUsOW"},"source":["if experiment_phase in {'training','continue'}:\n","    json_logging_callback = LambdaCallback(on_epoch_end=lambda epoch, logs:e_end('cosine_sim', epoch, logs, project_path+folder_path+'report.dat'))\n","    optimizer_weights_callback = LambdaCallback(on_epoch_end=save_optimizer_state(model, project_path+folder_path+'optimizer_weights.pkl'))\n","    model_weights_checkpoint = ModelCheckpoint(project_path+folder_path+'weights.h5', monitor='val_loss', verbose=0,save_best_only=True, mode='min', period=1, save_weights_only=True)\n","    early_stopping = EarlyStopping(monitor='val_loss', restore_best_weights=True,patience=2, mode='min',min_delta=0.001)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RWDiArvXU4sL"},"source":["if experiment_phase == 'continue':\n","    model.fit(x_train[:1000], y_train[:1000], epochs=1, batch_size=batch_size, validation_data=(x_dev, y_dev))\n","    with open(project_path+folder_path+'optimizer_weights.pkl') as f:\n","        optimizer_weights = pickle.load(f)\n","    model.optimizer.set_weights(optimizer_weights)   \n","    model.set_weights(project_path+folder_path+'weights.h5')\n","    start = time()\n","    history = model.fit(x_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(x_dev, y_dev)\n","    ,callbacks=[model_weights_checkpoint, early_stopping, json_logging_callback, optimizer_weights_callback]\n","    )   \n","    end = time()\n","    duration = round(end-start,2)\n","    write_line_to_file('Training Duration: '+str(duration)+' Seconds', log_dir)\n","elif experiment_phase == 'training':\n","    start = time()\n","    history = model.fit(x_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(x_dev, y_dev)\n","    ,callbacks=[model_weights_checkpoint, early_stopping, json_logging_callback, optimizer_weights_callback]\n","    )   \n","    end = time()\n","    duration = round(end-start,2)\n","    write_line_to_file('Training Duration: '+str(duration)+' Seconds', log_dir)\n","elif experiment_phase == 'querying':\n","    model.load_weights(project_path+folder_path+'weights.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HvCSOfTFPRWw"},"source":["# model.optimizer.set_weights(weight_values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r6Jx4uwK67RB"},"source":["if experiment_phase in {'training','continue'}:\n","    x_test = array([item['array'] for item in test])\n","    y_test = array([comparison_matrix[h2id[item['word']]] if item['word'] in h2id else output_vec_model[item['word']] for item in test])\n","    test_eval = model.evaluate(x_test, y_test)\n","    test_loss = test_eval[0]\n","    test_cosim = test_eval[1]\n","    log_str = 'Test Loss: '+str(test_loss)+' - '+'Cosine Similarity: ' + str(test_cosim)\n","    write_line_to_file(log_str, log_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lt0qyeJDha_J"},"source":["if experiment_phase in {'training','continue'}:\n","    plt.plot(history.history['cosine_sim'])\n","    plt.plot(history.history['val_cosine_sim'])\n","    plt.title('Training History')\n","    plt.ylabel('Cosine Similarity')\n","    plt.xlabel('epoch')\n","    plt.legend(['training', 'development'], loc='upper left')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nTtfuWPQaXQP"},"source":["if experiment_phase in {'training','continue'}:\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('Training History')\n","    plt.ylabel('Loss')\n","    plt.xlabel('epoch')\n","    plt.legend(['training', 'development'], loc='upper left')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v1NVpDTFX4MC"},"source":["**Evaluating the model based on its performance on the testing set**"]},{"cell_type":"code","metadata":{"id":"9uagVHoMKs49"},"source":["if experiment_phase in {'training','continue'}:\n","    train_samples = stratified_sample(train, {'amid','moeen-vy','dehkhoda-vy'}, num_head_words, acc_eval_sample_Size)\n","    train_inputs = {\n","        'definitions':[item['original_definition'] for item in train_samples],\n","        'words':[item['word'] for item in train_samples],\n","        'sources':[item['source'] for item in train_samples],\n","    }\n","    if 'attention_model' in globals():\n","        q = query(train_inputs, result_topn, max_seq_len, {'model':model, 'attention_model':attention_model}, tools)\n","    else:\n","        q = query(train_inputs, result_topn, max_seq_len, {'model':model}, tools)\n","    e = evaluate(q)\n","    write_line_to_file('-------------', log_dir)\n","    log_str = 'Seen Evaluation:\\n'+'Median: '+str(e['main_eval']['median'])+', Variance: '+str(e['main_eval']['variance'])+', Acc@10: '+str(e['main_eval']['acc@10'])+', Acc@100: '+str(e['main_eval']['acc@100'])+'\\n'\n","    write_line_to_file(log_str, log_dir)\n","    log_str = 'Seen Synonym Evaluation:\\n'+'Median: '+str(e['synset_eval']['median'])+', Variance: '+str(e['synset_eval']['variance'])+', Acc@10: '+str(e['synset_eval']['acc@10'])+', Acc@100: '+str(e['synset_eval']['acc@100'])+'\\n'\n","    write_line_to_file(log_str, log_dir)\n","    write_line_to_file(str(e['bad_results']), log_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DkFrpE5Vzm7w"},"source":["if experiment_phase in {'training','continue'}:\n","    test_samples = stratified_sample(test, {'amid','moeen-vy','dehkhoda-vy'}, num_head_words, acc_eval_sample_Size)\n","    test_inputs = {\n","        'definitions':[item['original_definition'] for item in test_samples],\n","        'words':[item['word'] for item in test_samples],\n","        'sources':[item['source'] for item in test_samples],\n","    }\n","    if 'attention_model' in globals():\n","        q = query(test_inputs, result_topn, max_seq_len, {'model':model, 'attention_model':attention_model}, tools)\n","    else:\n","        q = query(test_inputs, result_topn, max_seq_len, {'model':model}, tools)\n","    e = evaluate(q)\n","    write_line_to_file('-------------', log_dir)\n","    log_str = 'Unseen Evaluation:\\n'+'Median: '+str(e['main_eval']['median'])+', Variance: '+str(e['main_eval']['variance'])+', Acc@10: '+str(e['main_eval']['acc@10'])+', Acc@100: '+str(e['main_eval']['acc@100'])+'\\n'\n","    write_line_to_file(log_str, log_dir)\n","    log_str = 'Unseen Synonym Evaluation:\\n'+'Median: '+str(e['synset_eval']['median'])+', Variance: '+str(e['synset_eval']['variance'])+', Acc@10: '+str(e['synset_eval']['acc@10'])+', Acc@100: '+str(e['synset_eval']['acc@100'])+'\\n'\n","    write_line_to_file(log_str, log_dir)\n","    write_line_to_file(str(e['bad_results']), log_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y9TwgJ7hFYIv"},"source":["if experiment_phase in {'training','continue'}:\n","    synset_good10 = [item for item in q if 1<=item['synset_word_rank']<=10]\n","    synset_good100 = [item for item in q if 10<item['synset_word_rank']<=100]\n","    synset_bad100 = [item for item in q if 100<item['synset_word_rank']]\n","    save_group_reports(synset_good10, project_path+folder_path+'good10.html')\n","    save_group_reports(synset_good100, project_path+folder_path+'good100.html')\n","    save_group_reports(synset_bad100, project_path+folder_path+'bad100.html')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xA2Ixg6vevvl"},"source":["# if experiment_phase in {'training','continue'}:\n","#     dictionaries = {'amid','moeen-vy','dehkhoda-vy'}\n","#     for dictionary in dictionaries:\n","#         test_samples = stratified_sample(test, {dictionary}, num_head_words, mos_eval_sample_size)\n","#         test_inputs = {\n","#             'definitions':[item['original_definition'] for item in test_samples],\n","#             'words':[item['word'] for item in test_samples],\n","#             'sources':[item['source'] for item in test_samples]\n","#         }\n","#         if 'attention_model' in globals():\n","#             q = query(test_inputs, result_topn, max_seq_len, {'model':model, 'attention_model':attention_model}, tools)\n","#         else:\n","#             q = query(test_inputs, result_topn, max_seq_len, {'model':model}, tools)\n","#         e = evaluate(q)\n","#         write_line_to_file('-------------', log_dir)\n","#         log_str = 'Source: '+dictionary\n","#         write_line_to_file(log_str, log_dir)\n","#         log_str = 'Unseen Evaluation:\\n'+'Median: '+str(e['main_eval']['median'])+', Variance: '+str(e['main_eval']['variance'])+', Acc@10: '+str(e['main_eval']['acc@10'])+', Acc@100: '+str(e['main_eval']['acc@100'])+'\\n'\n","#         write_line_to_file(log_str, log_dir)\n","#         log_str = 'Unseen Synonym Evaluation:\\n'+'Median: '+str(e['synset_eval']['median'])+', Variance: '+str(e['synset_eval']['variance'])+', Acc@10: '+str(e['synset_eval']['acc@10'])+', Acc@100: '+str(e['synset_eval']['acc@100'])+'\\n'\n","#         write_line_to_file(log_str, log_dir)\n","#         write_line_to_file(str(e['bad_results']), log_dir)\n","#         correct_words = [item['main_word'] for item in q]\n","#         input_descriptions = [item['original_definition'] for item in q]\n","#         dict_evaluation_df = pd.DataFrame(list(zip(correct_words, input_descriptions)), columns=['word','description'])\n","#         dict_evaluation_df.to_excel(project_path+folder_path+dictionary+'.xlsx', encoding='utf-8')\n","#         for k in range(3):\n","#             output_words = [item['topwords'][k] for item in q]\n","#             evaluation_df = pd.DataFrame(list(zip(output_words, input_descriptions)), columns=['word','description'])\n","#             evaluation_df.to_excel(project_path+folder_path+dictionary+'-'+str(k+1)+'.xlsx', encoding='utf-8')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y_w7rs5WMu3Z","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601453237584,"user_tz":-210,"elapsed":18029,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"02018886-a8ed-4f23-a169-4f8abd9d05b4"},"source":["definition = 'حس خوشبختی که یک انسان دارد'\n","inputs = {'definitions': [definition],\n","          'words':['UNK']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: UNK</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: حس خوشبختی که یک انسان دارد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspحس&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspخوشبختی&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspیک&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspانسان&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: احساس -- آرامش -- حسی -- ناراحتی -- ذهنیت -- درد -- لذت -- عواطف -- ترس -- تنفر</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه UNK را در رتبه 20000 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (UNK) را در رتبه 20000 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"sDoYe_xQCsJw","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601453237585,"user_tz":-210,"elapsed":18016,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"c65dec9e-7cfc-4612-faab-18809e4686ce"},"source":["definition = 'ورزشی که بیشترین طرفدار را دارد'\n","inputs = {'definitions': [definition],\n","          'words':['فوتبال']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: فوتبال</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: ورزشی که بیشترین طرفدار را دارد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspورزشی&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspبیشترین&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspطرفدار&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: فوتبال -- بسکتبال -- استادیوم -- بوکس -- لیگ -- حریف -- بازیکن -- کشتی‌گیران -- دوومیدانی -- ژیمناستیک</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه فوتبال را در رتبه 1 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (فوتبال) را در رتبه 1 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"43xGWKN3D4XT","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601453396406,"user_tz":-210,"elapsed":765,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"35849fa6-cb1d-4f62-c986-f155d2488706"},"source":["definition = 'کسی که حالش خوش نیست'\n","inputs = {'definitions': [definition],\n","          'words':['بیمار']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: بیمار</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: کسی که حالش خوش نیست</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspکسی&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspحالش&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspخوش&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspنیست&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: دلنشین -- خندان -- بدبخت -- دلگیر -- رنجور -- ناراحت -- دلتنگ -- بیچاره -- خوشگل -- دیوانه</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه رنجور را در رتبه 5 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (بیمار) را در رتبه 425 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"Jt-SCPwiC_Ha","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601453987346,"user_tz":-210,"elapsed":933,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"2f8da635-ace7-4173-cc06-8bec5ef19c93"},"source":["definition = 'یک پرنده زیبا'\n","inputs = {'definitions': [definition],\n","          'words':['قناری']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: قناری</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: یک پرنده زیبا</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspیک&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspپرنده&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspزیبا&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: کلاغ -- بلبل -- گنجشک -- کاکلی -- عقاب -- فاخته -- طاووس -- غوک -- طوطی -- هدهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه قناری را در رتبه 85 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (قناری) را در رتبه 85 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"Ng7ojvk2EPvn","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601454117580,"user_tz":-210,"elapsed":888,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"44c2f56e-a1f6-465f-d933-66cc9034cb85"},"source":["definition = 'حل کننده مشکل ویروس کرونا'\n","inputs = {'definitions': [definition],\n","          'words':['واکسن']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: واکسن</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: حل کننده مشکل ویروس کرونا</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspحل&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspکننده&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspمشکل&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspویروس&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspUNK&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: حاد -- رنگدانه -- محلول -- عفونت -- قرحه -- مسئله -- عفونی -- مبتلا -- میکروب -- هپاتیت</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه واکسن را در رتبه 134 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (واکسن) را در رتبه 134 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"AKu64db5Goe3","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601454388310,"user_tz":-210,"elapsed":1151,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"a8c9e0eb-9250-4a19-ca03-a6befab9832a"},"source":["definition = 'تلاش جدی برای رسیدن به هدف'\n","inputs = {'definitions': [definition],\n","          'words':['ممارست']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: ممارست</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: تلاش جدی برای رسیدن به هدف</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspتلاش&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspجدی&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspرسیدن&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspهدف&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: تلاشی -- کوشش -- مبارزه -- ناکامی -- جدیت -- کشمکش -- عزم -- ناکام -- مصمم -- نبرد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه پشتکار را در رتبه 35 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (ممارست) را در رتبه 135 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"UPB8X0KJG22G","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601454529495,"user_tz":-210,"elapsed":1151,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"d2cb25fa-f175-4a24-84c7-bb4abb83c88e"},"source":["definition = 'هنر دروغ گفتن به مردم'\n","inputs = {'definitions': [definition],\n","          'words':['ریا']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: ریا</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: هنر دروغ گفتن به مردم</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspهنر&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspدروغ&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspگفتن&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspمردم&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: تهمت -- تزویر -- مضامین -- دورویی -- تحریف -- خرافه -- ریاکاری -- هنرمندانه -- حیله -- مسخرگی</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه تزویر را در رتبه 2 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (ریا) را در رتبه 9351 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"ersCxTJqINb9","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601455077466,"user_tz":-210,"elapsed":967,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"b1b96565-b20a-4e2e-9b2f-4a32973cbc97"},"source":["definition = 'دختر کوروش اول'\n","inputs = {'definitions': [definition],\n","          'words':['آتوسا']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: آتوسا</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: دختر کوروش اول</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspدختر&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspکوروش&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspاول&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: مریم -- مهشید -- ملیکا -- آتوسا -- فاطمه -- ملیحه -- بردیا -- الهه -- میترا -- ناهید</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه آتوسا را در رتبه 4 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (آتوسا) را در رتبه 4 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"Hd2WRw_AImfX","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601455833058,"user_tz":-210,"elapsed":1833,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"62ba5245-c435-4cd4-d423-c2990dc5f0d4"},"source":["definition = 'اولین پایتخت رسمی ایران'\n","inputs = {'definitions': [definition],\n","          'words':['همدان']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: همدان</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: اولین پایتخت رسمی ایران</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspاولین&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspپایتخت&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspرسمی&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspایران&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: قرقیزستان -- فیلیپین -- کویت -- پاکستان -- ونزوئلا -- سنگاپور -- اندونزی -- تایلند -- مراکش -- کامبوج</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه همدان را در رتبه 222 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (همدان) را در رتبه 222 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"P5-9NIX8NLhd","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601455948216,"user_tz":-210,"elapsed":1197,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"b1b90795-8cd2-41a7-9b1b-a1b9c94e5415"},"source":["definition = 'آوازش از دور خوش است'\n","inputs = {'definitions': [definition],\n","          'words':['دهل']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: دهل</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: آوازش از دور خوش است</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspآوازش&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspدور&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspخوش&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: دلنشین -- دلپذیر -- شورانگیز -- لعبت -- آمیغ -- عارفانه -- گشادن -- زیبا -- نغمه -- بلبل</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه دهل را در رتبه 1085 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (دهل) را در رتبه 1085 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"fgM8D6x0Nnyj","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601456336364,"user_tz":-210,"elapsed":2137,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"c7df25db-9c05-4ed3-ea74-cd580e38e6cd"},"source":["definition = 'از بدترین کارهایی که یک انسان بخاطر پول ممکن است انجام بدهد'\n","inputs = {'definitions': [definition],\n","          'words':['اختلاس']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: اختلاس</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: از بدترین کارهایی که یک انسان بخاطر پول ممکن است انجام بدهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspبدترین&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspکارهایی&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspیک&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspانسان&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspبخاطر&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspپول&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspممکن&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspانجام&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspبدهد&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: مخارج -- پولی -- تنگدستی -- خرج -- هزینه -- وجوه -- روپیه -- مبالغ -- احتکار -- صدقه</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اختلاس را در رتبه 14 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (اختلاس) را در رتبه 14 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"LzU1_vvoN4Fk","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601457026753,"user_tz":-210,"elapsed":1948,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"7fead5c3-2506-4fff-b48b-3b5cc5836f25"},"source":["definition = 'کسی که از مالیات فرار می‌کند'\n","inputs = {'definitions': [definition],\n","          'words':['UNK']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: UNK</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: کسی که از مالیات فرار می‌کند</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspکسی&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspمالیات&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspفرار&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspمی‌کند&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: متواری -- ستمکار -- ستدن -- بیچاره -- ترسانیدن -- یاغی -- گروگان -- نوکر -- تنگدستی -- ستیز</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه UNK را در رتبه 20000 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (UNK) را در رتبه 20000 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"3GaQThFVPoIT","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601457950291,"user_tz":-210,"elapsed":1125,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"425e4de3-11ce-4404-c8d2-5066e82d9520"},"source":["definition = 'تجمع مسالمت‌آمیز در اعتراض به مسائل سیاسی'\n","inputs = {'definitions': [definition],\n","          'words':['تحصن']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">کلمه اصلی: تحصن</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">تعریف واقعی: تجمع مسالمت‌آمیز در اعتراض به مسائل سیاسی</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">ورودی مدل: <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspتجمع&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspمسالمت‌آمیز&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspاعتراض&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspمسائل&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.5%)\">&nbspسیاسی&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">نقش دستوری: </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">خروجی مدل: تحصن -- اعتراضات -- سرکوب -- شورش -- ضدیت -- حکومت -- سرنگونی -- مخالفت -- تعرض -- کودتا</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه تحصن را در رتبه 1 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">مدل کلمه اصلی (تحصن) را در رتبه 1 می دهد</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"CfZeYEcySUbS"},"source":[""],"execution_count":null,"outputs":[]}]}