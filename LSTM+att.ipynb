{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM+att.ipynb","provenance":[{"file_id":"16LC16BtCxDitLEXzZlj5xDswL-v6NFI3","timestamp":1600883903974}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"xZRYVm9y1eAC"},"source":["from google.colab import drive\n","import warnings"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FxZH0CrXWAlN"},"source":["**Authentication**"]},{"cell_type":"code","metadata":{"id":"y7fiSgckBIpF","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1601453301841,"user_tz":-210,"elapsed":3916,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"347c67f5-57e3-4b1c-e972-b67b86048694"},"source":["drive.mount('/gdrive', force_remount=True)\n","%cd /gdrive/My Drive/Persian-Reverse-Dictionary/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n","/gdrive/My Drive/Persian-Reverse-Dictionary\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bMzxZkFjWHca"},"source":["**Importing the necessary libraries**"]},{"cell_type":"code","metadata":{"id":"wQ224cPaBO8L"},"source":["from experiment_v8 import *\n","from globals import *\n","from attention import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_lup1Ja_WcPF"},"source":["**Configuration**"]},{"cell_type":"code","metadata":{"id":"CLytgx5NCDvq"},"source":["np.set_printoptions(suppress=True)\n","experiment_phase = 'querying' # phase: training/querying/continue\n","warnings.filterwarnings(\"ignore\")\n","random_seed = 1234\n","tf.random.set_seed(random_seed)\n","seed(random_seed)\n","shared_files = read_csv_as_dict('shared_files.csv')\n","globals().update(shared_files)\n","model_architecture = 'lstmatt'\n","project_path = 'models/'+model_architecture+'/'\n","num_head_words = 20000\n","folder_path = str(num_head_words)+'/'\n","gloss_max_rank = 100000\n","input_emb_size = 300\n","lstm_output_size = 300\n","dense_output_size = 300\n","output_emb_size = 300\n","pretrained_input = True\n","fixed_embeddings = False\n","pretrained_target = True\n","normalize_vectors = False\n","add_context = False\n","augment_head = False\n","augment_gloss = False\n","save_train_test_samples = False\n","save_tools = False\n","save_weights = True\n","use_intent_classifier = False\n","max_hfake_per_sample = 5\n","gfake_per_sample = {\n","    3:5,\n","    4:5\n","}\n","context_max_rank = 100000\n","max_seq_len = 20\n","learning_rate = 1.0\n","margin = 1.0\n","num_epochs = 50\n","batch_size = 16\n","mos_eval_sample_size = 40\n","result_topn = 10\n","acc_eval_sample_Size = 500\n","input_vector_model = 'fasttext'\n","output_vector_model = 'fasttext'\n","data_sources = {'wikipedia','amid','dehkhoda-vy','moeen-vy','farsnet'}\n","vector_model_dir = {'fasttext':fasttext_dict_dir,'irblog':irblog2_wv_dir,'wiki200':wiki_wv_200_dir,'hamwv':hamshahri_wv_dir,'hamft':hamshahri_ft_dir,'wordak':wordak_wv_dir, 'wiki300':wiki_wv_300_dir,'twitter':twitter_wv_dir}\n","max_sense = {\n","    'wikipedia':500,\n","    'amid':500,\n","    'dehkhoda-vy':500,\n","    'moeen-vy':500,\n","    'farsnet':500\n","}\n","num_heads = 8\n","d_model = 300\n","loss_function = 'cosine' # cosine, ranking\n","intent_project_path = 'July/2020-7-23/'\n","intent_folder_path = 'exp9/'\n","intent_maxlen = 20\n","intent_threshold = 0.3\n","intent_coefficient = 0.4\n","vec_sim_coefficient = 0.6"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WSYzfzFTHopD"},"source":["if experiment_phase == 'training':\n","    log_dir = project_path+folder_path+'report.dat'\n","    log_obj = {\n","        'gloss':str(gloss_max_rank),\n","        'head':str(num_head_words),\n","        'input_emb':str(input_emb_size),\n","        'output_emb':str(output_emb_size),\n","        'dense_output':str(dense_output_size),\n","        'lstm_output':str(lstm_output_size),\n","        'pretrained_input':input_vector_model if pretrained_input else 'no',\n","        'pretrained_target':output_vector_model if pretrained_target else 'no',\n","        'fixed_embeddings':'yes' if fixed_embeddings else 'no',\n","        'normalize_vectors':'yes' if normalize_vectors else 'no',\n","        'intent_classifier':'yes' if use_intent_classifier else 'no',\n","        'context':str(context_max_rank) if add_context else 'no',\n","        'augment_head':str(max_hfake_per_sample) if augment_head else 'no',\n","        'augment_gloss':str(gfake_per_sample) if augment_gloss else 'no',\n","        'margin':str(margin),\n","        'seq_len':str(max_seq_len),\n","        'learning_rate':str(learning_rate),\n","        'batch_size':str(batch_size),\n","        'data_sources':\" \".join(data_sources),\n","        'max_sense':str(max_sense),\n","        'model_architecture':model_architecture,\n","        'loss_function':loss_function,\n","        'intent_path':intent_project_path+intent_folder_path,\n","        'intent_maxlen':str(intent_maxlen),\n","        'intent_threshold':str(intent_threshold),\n","        'intent_coefficient':str(intent_coefficient),\n","        'vec_sim_coefficient':str(vec_sim_coefficient)\n","    }\n","    init_log(log_obj, log_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Dhnb4tXWtDP"},"source":["**Loading**<br/>\n","* Data \n","* Stopwords\n","* Normalizer\n","* Ranking of words by frequency\n","* Synonyms Set\n","* POS Tags\n","* Vector Model(s)"]},{"cell_type":"code","metadata":{"id":"4Fc3W5BaNM4-"},"source":["word_tags = read_pickle(wordtags_dir) if use_intent_classifier else defaultdict(set)\n","ranking = read_pickle(ranking_dir)\n","all_words = set(ranking)\n","stopwords = read_lines(stopwords_dir)\n","normalizer = read_json(normalizer_dir)\n","synonyms = read_pickle(synonyms_dir)\n","if experiment_phase in {'training','continue'}:\n","    data = read_pickle(data_dir)\n","input_vec_model = read_pickle(vector_model_dir[input_vector_model])\n","output_vec_model = read_pickle(vector_model_dir[input_vector_model])\n","if use_intent_classifier == True:\n","    with open(intent_project_path+intent_folder_path+'model.json','r') as json_file:\n","        intent_model_arch = json_file.read()\n","    intent_model = tf.keras.models.model_from_json(intent_model_arch)\n","    intent_model.load_weights(intent_project_path+intent_folder_path+'weights.h5')\n","    intent_id2c = read_pickle(intent_project_path+intent_folder_path+'id2c.pkl')\n","    intent_g2id = read_pickle(intent_project_path+intent_folder_path+'intent-g2id.pkl')\n","    tools = {'normalizer':normalizer,'stopwords':stopwords,'synonyms':synonyms,'word_tags':word_tags, 'intent_g2id':intent_g2id, 'intent_maxlen':intent_maxlen, 'intent_model':intent_model, 'intent_threshold':intent_threshold, 'intent_id2c':intent_id2c, 'intent_coefficient':intent_coefficient, 'vec_sim_coefficient':vec_sim_coefficient, 'output_emb_size':output_emb_size}\n","else:\n","    tools = {'normalizer':normalizer,'stopwords':stopwords,'synonyms':synonyms,'word_tags':word_tags, 'output_emb_size':output_emb_size}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pB24i9DjCopu"},"source":["items = []\n","sources_counter = Counter()\n","word_rank = {word:idx+1 for idx, word in enumerate(ranking)}\n","if experiment_phase in {'training','continue'}:\n","    for item in data:\n","        sources_counter[item['dic']] += 1\n","        new_item = {'word':item['word'],\n","                    'rank':word_rank[item['word']]+1 if item['word'] in all_words else len(ranking)+1,\n","                    'original_definition':item['meaning'],\n","                    'preprocessed_definition':item['meaning'],\n","                    'active':True,\n","                    'array':None,\n","                    'context':False,\n","                    'type':'main',\n","                    'sense_tags':item['tags'] if 'tags' in item else set(),\n","                    'general_tags':item['general_tags'] if 'general_tags' in item else set(),\n","                    'sid':item['sense_id'],\n","                    'source':item['dic'],\n","                    'phase':item['phase']\n","                    }\n","        items.append(new_item)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5VzY8W9_XFQh"},"source":["**Preparing the Train, Test and Dev Sets**"]},{"cell_type":"code","metadata":{"id":"B01R_HZH8WKA"},"source":["if experiment_phase in {'training','continue'}:\n","    ## fixing the data sources and senses\n","    for item in items:\n","        if item['source'] not in data_sources or (item['sid']>max_sense[item['source']]):\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    ## removing html tags\n","    for item in items:\n","        item['word'] = remove_html_tags(item['word'])\n","        item['original_definition'] = remove_html_tags(item['original_definition'])\n","        item['preprocessed_definition'] = remove_html_tags(item['preprocessed_definition'])\n","    ## deactivating samples without any Persian information\n","    for item in items:\n","        if not (has_any_persian(item['word']) and has_any_persian(item['original_definition'])):\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'with Persian information - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","    ## normalizing characters\n","    for item in items:\n","        item['preprocessed_definition'] = normalize_characters(item['preprocessed_definition'], normalizer)\n","        item['word'] = normalize_characters(item['word'], normalizer)\n","    ## correcting whitespaces\n","    for item in items:\n","        item['word'] = correct_whitespaces(item['word'])\n","        item['preprocessed_definition'] = correct_whitespaces(item['preprocessed_definition'])\n","    ## tokenization\n","    for item in items:\n","        item['preprocessed_definition'] = tokenize(item['preprocessed_definition'])\n","    ## removing self-definition(s)\n","    for item in items:\n","        item['preprocessed_definition'] = remove_self_definition(item['preprocessed_definition'], item['word'])\n","    ## removing stopwords\n","    for item in items:\n","        item['preprocessed_definition'] = remove_stopwords(item['preprocessed_definition'], stopwords)\n","    for item in items:\n","        if item['word'] in stopwords:\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'Stopwords removed - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","    ## deactivating unlearnables\n","    for item in items:\n","        if not has_vector(item['word'], output_vec_model):\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'Unlearnables Removed - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","    ## deactivating short words\n","    for item in items:\n","        if len(item['word']) < 3:\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'Short words removed - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","    ## deactivating short definitions\n","    for item in items:\n","        if len(item['preprocessed_definition']) == 1 and len(item['preprocessed_definition'][0]) < 3:\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'Short definitions removed - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","    ## deactivating meaningless words\n","    for item in items:\n","        if len(item['preprocessed_definition']) == 0:\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'Meaningless words removed - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","    ## normalizing heads by ranking (frequency)\n","    active_words = set([item['word'] for item in items if item['active']==True])\n","    temp_ranking = [word for word in ranking if word in active_words]\n","    frequent_words = set([word for word in temp_ranking][:num_head_words])\n","    context_words = set([word for word in temp_ranking][:context_max_rank])\n","    for item in items:\n","        if item['word'] not in frequent_words:\n","            if add_context == True:\n","                if item['word'] in context_words:\n","                    item['phase'] = 'train'\n","                    item['context'] = True\n","                else:\n","                    item['active'] = False\n","            else:\n","                item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    main_words = set([item['word'] for item in items if item['context']==False])\n","    log_str = 'Normalized heads by frequency - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words)) +' ('+str(len(main_words))+' Main Words)'\n","    write_line_to_file(log_str, log_dir)\n","    ## generating the comparison matrix\n","    h2id = {}\n","    id2h = {}\n","    for i, h in enumerate(frequent_words):\n","        h2id[h] = i\n","        id2h[i] = h\n","    comparison_matrix = zeros((num_head_words, output_emb_size))\n","    if pretrained_target == True:\n","        for item in items:\n","            if item['phase'] == 'train' and item['word'] in frequent_words:\n","                comparison_matrix[h2id[item['word']]] = output_vec_model[item['word']]\n","    ## normalizing the tokens based on their frequency\n","    tokens_lst = []\n","    for item in items:\n","        if item['phase'] == 'train':\n","            tokens_lst.extend(item['preprocessed_definition'])\n","    frequent_tokens = most_frequent(tokens_lst, gloss_max_rank)\n","    for item in items:\n","        item['preprocessed_definition'] = [token if token in frequent_tokens else 'UNK' for token in item['preprocessed_definition']]\n","        if 'UNK' in item['preprocessed_definition']:\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    main_words = set([item['word'] for item in items if item['context']==False])\n","    log_str = 'Normalized tokens by frequency - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words)) +' ('+str(len(main_words))+' Main Words)'\n","    write_line_to_file(log_str, log_dir)\n","    ## encoding the tokens\n","    t2id = {}\n","    id2t = {}\n","    t2id['PAD'] = 0\n","    t2id['UNK'] = 1\n","    id2t[0] = 'PAD'\n","    id2t[1] = 'UNK' \n","    for i, t in enumerate(frequent_tokens, start=2):\n","        t2id[t] = i\n","        id2t[i] = t\n","    ## generating the embedding matrix\n","    embedding_matrix = zeros((len(frequent_tokens)+2, input_emb_size))\n","    if pretrained_input == True:\n","        for token in frequent_tokens:\n","            try:\n","                embedding_matrix[t2id[token]] = input_vec_model[token]\n","                if normalize_vectors == True and LA.norm(embedding_matrix[t2id[token]])>0.0:\n","                    embedding_matrix[t2id[token]] = embedding_matrix[t2id[token]]/LA.norm(embedding_matrix[t2id[token]])\n","            except:\n","                continue\n","\n","    ## fixing the length of sequences\n","    for item in items:\n","        tokens = item['preprocessed_definition']\n","        item['preprocessed_definition'] = tokens[:max_seq_len] if len(tokens)>=max_seq_len else tokens+['PAD' for j in range(max_seq_len-len(tokens))]\n","    ## removing the words with definitions that are only consisted of 'PAD' and 'UNK'\n","    for item in items:\n","        tokens = item['preprocessed_definition']\n","        tokens = set(tokens)\n","        if tokens.issubset({'PAD','UNK'}):\n","            item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    log_str = 'Infoless definitions (UNK and PAD) removed - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))\n","    write_line_to_file(log_str, log_dir)\n","\n","    ## removing duplicates (samples with equal word and definition)\n","    # from the training data\n","    sample_count = Counter()\n","    for item in items:\n","        if item['phase'] == 'train':\n","            word = item['word']\n","            tokens = item['preprocessed_definition']\n","            sample_key = word+'-'+\"-\".join(tokens)\n","            sample_count[sample_key] += 1\n","            if sample_count[sample_key] > 1:\n","                item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    main_words = set([item['word'] for item in items if item['context']==False])\n","    log_str = 'duplicates removed from training data - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))+' ('+str(len(main_words))+' Main Words)'\n","    write_line_to_file(log_str, log_dir)\n","    # from the testing and development data\n","    for item in items:\n","        if item['phase'] != 'train':\n","            word = item['word']\n","            tokens = item['preprocessed_definition']\n","            sample_key = word+'-'+\"-\".join(tokens)\n","            sample_count[sample_key] += 1\n","            if sample_count[sample_key] > 1:\n","                item['active'] = False\n","    items = [item for item in items if item['active'] == True]\n","    all_words = set([item['word'] for item in items])\n","    main_words = set([item['word'] for item in items if item['context']==False])\n","    log_str = 'duplicates removed from unseen data - Number of Samples: '+str(len(items))+' - Number of Words: '+str(len(all_words))+' ('+str(len(main_words))+' Main Words)'\n","    write_line_to_file(log_str, log_dir)\n","    ## generating the data\n","    for item in items:\n","        tokens = item['preprocessed_definition']\n","        nparray = [t2id[token] for token in tokens]\n","        nparray = array(nparray)\n","        item['array'] = nparray\n","    train = [item for item in items if item['phase'] == 'train']\n","    test = [item for item in items if item['phase'] == 'test']\n","    dev = [item for item in items if item['phase'] == 'dev']\n","    log_str = 'After Preprocessing: '+str(len(train))+' training samples, '+str(len(test))+' testing samples and '+str(len(dev))+' samples for development'\n","    write_line_to_file(log_str, log_dir)\n","    tools_part2 = {'t2id':t2id,'id2t':id2t,'h2id':h2id,'id2h':id2h,'embedding_matrix':embedding_matrix,'comparison_matrix':comparison_matrix}\n","    tools.update(tools_part2)\n","    train_sources_counter = Counter()\n","    for item in train:\n","        train_sources_counter[item['source']] += 1\n","    test_sources_counter = Counter()\n","    for item in test:\n","        test_sources_counter[item['source']] += 1\n","    dev_sources_counter = Counter()\n","    for item in dev:\n","        dev_sources_counter[item['source']] += 1\n","    write_line_to_file('Training Data: '+str(sum(train_sources_counter.values())), log_dir)\n","    write_line_to_file(str(train_sources_counter), log_dir)\n","    write_line_to_file('Development Data: '+str(sum(dev_sources_counter.values())), log_dir)\n","    write_line_to_file(str(dev_sources_counter), log_dir)\n","    write_line_to_file('Testing Data: '+str(sum(test_sources_counter.values())), log_dir)\n","    write_line_to_file(str(test_sources_counter), log_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XOsqxIYlGMZ7"},"source":["# if experiment_phase == 'training':\n","#     words = list(set(list(h2id.keys())))\n","#     with open(project_path+folder_path+'words'+str(num_head_words)+'.pkl','wb') as file:\n","#         pickle.dump(words, file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRtgkwzAWtGn"},"source":["if experiment_phase in {'training','continue'}:\n","    from copy import deepcopy\n","    # Making a counter to see if a sample already exists\n","    item_count = Counter()\n","    for item in items:\n","        word = item['word']\n","        tokens = item['preprocessed_definition']\n","        definition = \" \".join(tokens)\n","        key = word+'-'+definition\n","        item_count[key] += 1\n","\n","    # Augmenting (head words)\n","\n","    head_augment = []\n","    all_heads = set([item['word'] for item in items if item['phase']=='train'])\n","    replaceable_heads = set([word for word in all_heads if word in synonyms])\n","    if augment_head == True:\n","        for item in train:\n","            if len(train) % 100000 == 0:\n","                print(len(train))\n","            word = item['word']\n","            tokens = item['preprocessed_definition']\n","            definition = \" \".join(tokens)\n","            syn_words = synonyms[word]\n","            syn_words = set([w for w in syn_words if w in output_vec_model and w != word])\n","            if len(syn_words) == 0:\n","                continue\n","            if len(syn_words) >= max_hfake_per_sample:\n","                syn_candidates = sample(syn_words, max_hfake_per_sample)\n","            else:\n","                syn_candidates = list(syn_words)\n","            for candidate in syn_candidates:\n","                new_item = item.copy()\n","                new_item['word'] = candidate\n","                new_item['type'] = 'augment'\n","                key = candidate+'-'+definition\n","                if item_count[key] == 0:\n","                    head_augment.append(new_item)\n","                    item_count[key] += 1\n","\n","    # Augmenting (gloss words)\n","\n","    gloss_augment = []\n","\n","\n","    usable_synonyms = defaultdict()\n","    for (word, synset) in synonyms.items():\n","        temp_synset = set([syn for syn in synset if syn != word and syn in t2id])\n","        if len(temp_synset) > 0:\n","            usable_synonyms[word] = temp_synset\n","\n","    gloss_augment = []\n","\n","    if augment_gloss == True:\n","        for item in train:\n","            \n","            if item['type'] != 'main':\n","                continue\n","            word = item['word']\n","            tokens = item['preprocessed_definition']\n","            definition = \" \".join(tokens)\n","            rep_tokens = set([token for token in tokens if token in usable_synonyms and token in input_vec_model])\n","            for num_tokens_to_replace in gfake_per_sample.keys():\n","                if len(rep_tokens) < num_tokens_to_replace:\n","                    continue\n","                remaining_samples = gfake_per_sample[num_tokens_to_replace]\n","                remaining_tries = 50\n","                while remaining_samples > 0 and remaining_tries > 0:\n","                    chosen_to_be_changed = set(sample(rep_tokens, num_tokens_to_replace))\n","                    what_to_replace_with = defaultdict()\n","                    for token in chosen_to_be_changed:\n","                        candidates = usable_synonyms[token]\n","                        what_to_replace_with[token] = sample(candidates, 1)[0]\n","                    new_item = deepcopy(item)\n","                    new_item['preprocessed_definition'] = [what_to_replace_with[t] if t in chosen_to_be_changed else t for t in new_item['preprocessed_definition']]\n","                    new_item['array'] = np.array([t2id[t] for t in new_item['preprocessed_definition']])\n","                    new_item_key = new_item['word'] + '-' + \" \".join(new_item['preprocessed_definition'])\n","                    remaining_tries -= 1\n","                    if item_count[new_item_key] == 0:\n","                        gloss_augment.append(new_item)\n","                        item_count[new_item_key] += 1\n","                        remaining_samples -= 1\n","\n","\n","    train.extend(head_augment)\n","    del head_augment\n","    log_str = 'finished augmentation (head) - '+str(len(train))+' samples'\n","    write_line_to_file(log_str, log_dir)\n","    train.extend(gloss_augment)\n","    del gloss_augment\n","    log_str = 'finished augmentation (gloss) - '+str(len(train))+' samples'\n","    write_line_to_file(log_str, log_dir)\n","    del items"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JDRxe3_DeGMn"},"source":["if experiment_phase == 'training' and save_train_test_samples == True:\n","    train_data = defaultdict(list)\n","    test_data = defaultdict(list)\n","    train_original_data = defaultdict(list)\n","    test_original_data = defaultdict(list)\n","    for item in train:\n","        word = item['word']\n","        original_definition = \" \".join(item['original_definition'].split()[:max_seq_len])\n","        definition = \" \".join([token for token in item['preprocessed_definition'] if not token == 'PAD'])\n","        train_data[word].append(definition)\n","        train_original_data[word].append(original_definition)\n","    for item in test:\n","        word = item['word']\n","        original_definition = \" \".join(item['original_definition'].split()[:max_seq_len])\n","        definition = \" \".join([token for token in item['preprocessed_definition'] if not token == 'PAD'])\n","        test_data[word].append(definition)\n","        test_original_data[word].append(original_definition)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"exyXpM20Sh7Y"},"source":["if experiment_phase in {'training','continue'}:\n","    x_train = array([item['array'] for item in train])\n","    y_train = array([comparison_matrix[h2id[item['word']]] if item['word'] in h2id else output_vec_model[item['word']] for item in train])\n","    x_dev = array([item['array'] for item in dev])\n","    y_dev = array([comparison_matrix[h2id[item['word']]] if item['word'] in h2id else output_vec_model[item['word']] for item in dev])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qTHzcwVEXQ3k"},"source":["**Saving the necessary variables**"]},{"cell_type":"code","metadata":{"id":"CAMMkkIVXR6V"},"source":["if experiment_phase == 'training':\n","    if save_tools == True:\n","        with open(project_path+folder_path+'tools.pkl','wb') as file:\n","            pickle.dump(tools, file)\n","    if save_train_test_samples == True:\n","        train_test_data = (train_data, train_original_data, test_data, test_original_data)\n","        with open(project_path+folder_path+'train_test_data.pkl','wb') as file:\n","            pickle.dump(train_test_data, file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mx7KX9xvKhPi"},"source":["if experiment_phase == 'training':\n","    emb_matrix_shape = embedding_matrix.shape\n","    additional_tools = {'comparison_matrix':comparison_matrix, 't2id':t2id, 'h2id':h2id, 'id2h':id2h}\n","    query_tools = (emb_matrix_shape, additional_tools)\n","    with open(project_path+folder_path+'query_tools.pkl','wb') as file:\n","        pickle.dump(query_tools, file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F_gBuN56Ivew"},"source":["**Loading Necessary Tools for Querying**"]},{"cell_type":"code","metadata":{"id":"bWelxKZ2KJeJ"},"source":["if experiment_phase == 'querying':\n","    emb_matrix_shape , additional_tools = read_pickle(project_path+folder_path+'query_tools.pkl')\n","    tools.update(additional_tools)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WsWo-X61Xiku"},"source":["**Making a Model and Training it**"]},{"cell_type":"markdown","metadata":{"id":"16aucIuAj_Gy"},"source":["*Bag of Words*"]},{"cell_type":"code","metadata":{"id":"pRZhlJhpiipI"},"source":["if model_architecture == 'bow':\n","    sequence_input = Input(shape=(max_seq_len,), dtype='int32', name='input')\n","    embedding_layer = Embedding(emb_matrix_shape[0],\n","                                input_emb_size,\n","                                weights=[embedding_matrix] if 'embedding_matrix' in globals() else [np.zeros(emb_matrix_shape)],\n","                                input_length=max_seq_len,\n","                                trainable=not fixed_embeddings,\n","                                mask_zero=True, name='embedding')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    mean_vector = Lambda(lambda x: mean(x, axis=1), name='lambda')(embedded_sequences)\n","    mean_vector = Flatten(name='flatten')(mean_vector)\n","    output = Dense(dense_output_size, activation='tanh', name='dense')(mean_vector)\n","    adadelta = optimizers.Adadelta(lr=learning_rate)\n","    model = Model(inputs=sequence_input, outputs=output)\n","    if loss_function == 'cosine':\n","        model.compile(loss=cosine_loss,\n","                    optimizer=adadelta,metrics=[cosine_sim])\n","    elif loss_function == 'ranking':\n","        model.compile(loss=rank_loss(margin),\n","                    optimizer=adadelta,metrics=[cosine_sim])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2o1UORkqk6Ed"},"source":["*LSTM (without attention)*"]},{"cell_type":"code","metadata":{"id":"hGT_QZftoYWw"},"source":["if model_architecture == 'lstm':\n","    sequence_input = Input(shape=(max_seq_len,), dtype='int32', name='input')\n","    embedding_layer = Embedding(emb_matrix_shape[0],\n","                                input_emb_size,\n","                                weights=[embedding_matrix]  if 'embedding_matrix' in globals() else [np.zeros(emb_matrix_shape)],\n","                                input_length=max_seq_len,\n","                                trainable=not fixed_embeddings,\n","                                mask_zero=True, name='embedding')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    lstm = LSTM(lstm_output_size, return_sequences=False, name='lstm')(embedded_sequences)\n","    output = Dense(dense_output_size, activation='tanh', name='dense')(lstm)\n","    adadelta = optimizers.Adadelta(lr=learning_rate)\n","    model = Model(inputs=sequence_input, outputs=output)\n","    if loss_function == 'cosine':\n","        model.compile(loss=cosine_loss,\n","                    optimizer=adadelta,metrics=[cosine_sim])\n","    elif loss_function == 'ranking':\n","        model.compile(loss=rank_loss(margin),\n","                    optimizer=adadelta,metrics=[cosine_sim])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FkWxjSIZlwAA"},"source":["*BiLSTM (without attention)*"]},{"cell_type":"code","metadata":{"id":"6SWCnEdXlyU0"},"source":["if model_architecture == 'bilstm':\n","    sequence_input = Input(shape=(max_seq_len,), dtype='int32', name='input')\n","    embedding_layer = Embedding(emb_matrix_shape[0],\n","                                input_emb_size,\n","                                weights=[embedding_matrix] if 'embedding_matrix' in globals() else [np.zeros(emb_matrix_shape)],\n","                                input_length=max_seq_len,\n","                                trainable=not fixed_embeddings,\n","                                mask_zero=True, name='embedding')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    lstm = Bidirectional(LSTM(lstm_output_size, return_sequences=False, name='lstm'))(embedded_sequences)\n","    output = Dense(dense_output_size, activation='tanh', name='dense')(lstm)\n","    adadelta = optimizers.Adadelta(lr=learning_rate)\n","    model = Model(inputs=sequence_input, outputs=output)\n","    if loss_function == 'cosine':\n","        model.compile(loss=cosine_loss,\n","                    optimizer=adadelta,metrics=[cosine_sim])\n","    elif loss_function == 'ranking':\n","        model.compile(loss=rank_loss(margin),\n","                    optimizer=adadelta,metrics=[cosine_sim])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CBxsYqtk1z6J"},"source":["*LSTM (with attention)*"]},{"cell_type":"code","metadata":{"id":"N_kTQtT2Wkmk"},"source":["if model_architecture == 'lstmatt':\n","    sequence_input = Input(shape=(max_seq_len,), dtype='int32', name='input')\n","    embedding_layer = Embedding(emb_matrix_shape[0],\n","                                input_emb_size,\n","                                weights=[embedding_matrix] if 'embedding_matrix' in globals() else [np.zeros(emb_matrix_shape)],\n","                                input_length=max_seq_len,\n","                                trainable=not fixed_embeddings,\n","                                mask_zero=True, name='embedding')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    lstm, state_h, state_c = LSTM(lstm_output_size, return_sequences=True, return_state=True, name='lstm')(embedded_sequences)\n","    state_h = Lambda(lambda x: tf.expand_dims(x, 1), name='expand_dims')(state_h)\n","    state_h = Dense(300, name='d1')(state_h)\n","    lstm = Dense(300, name='d2')(lstm)\n","    weights, context_vector = AdditiveAttention(name='attention')([state_h, lstm])\n","    weights = Lambda(lambda x: tf.squeeze(x), name='squeezed_weights')(weights)\n","    output = Dense(dense_output_size, activation='tanh', name='dense')(context_vector)\n","    output = Lambda(lambda x: tf.squeeze(x), name='squeezed_output')(output)\n","    adadelta = optimizers.Adadelta(lr=learning_rate)\n","    model = Model(inputs=sequence_input, outputs=output)\n","    attention_model = Model(inputs=sequence_input, outputs=weights)\n","    if loss_function == 'cosine':\n","        model.compile(loss=cosine_loss,\n","                    optimizer=adadelta,metrics=[cosine_sim])\n","    elif loss_function == 'ranking':\n","        model.compile(loss=rank_loss(margin),\n","                    optimizer=adadelta,metrics=[cosine_sim])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bqiOp6yVl_Fe"},"source":["*BiLSTM (with attention)*"]},{"cell_type":"code","metadata":{"id":"z_CcCLEK0r8O"},"source":["if model_architecture == 'bilstmatt':\n","    sequence_input = Input(shape=(max_seq_len,), dtype='int32', name='input')\n","    embedding_layer = Embedding(emb_matrix_shape[0],\n","                                input_emb_size,\n","                                weights=[embedding_matrix] if 'embedding_matrix' in globals() else [np.zeros(emb_matrix_shape)],\n","                                input_length=max_seq_len,\n","                                trainable=not fixed_embeddings,\n","                                mask_zero=True, name='embedding')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(lstm_output_size, return_sequences=True, return_state=True, name='lstm'))(embedded_sequences)\n","    state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n","    state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n","    state_h = tf.expand_dims(state_h, 1)\n","    state_h = Dense(300, name='d1')(state_h)\n","    lstm = Dense(300, name='d2')(lstm)\n","    weights, context_vector = AdditiveAttention()([state_h, lstm])\n","    weights = tf.squeeze(weights)\n","    output = Dense(dense_output_size, activation='tanh', name='dense')(context_vector)\n","    output = tf.squeeze(output)\n","    adadelta = optimizers.Adadelta(lr=learning_rate)\n","    model = Model(inputs=sequence_input, outputs=output)\n","    attention_model = Model(inputs=sequence_input, outputs=weights)\n","    if loss_function == 'cosine':\n","        model.compile(loss=cosine_loss,\n","                    optimizer=adadelta,metrics=[cosine_sim])\n","    elif loss_function == 'ranking':\n","        model.compile(loss=rank_loss(margin),\n","                    optimizer=adadelta,metrics=[cosine_sim])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VUXgFCzbhTo"},"source":["# lstm: [2, 20, 600]\n","# state_h: [2, 600]\n","# new_lstm (after Dense(300)):  [2, 20, 300]\n","# new_state_h (after expand_dims): [2, 1, 600]\n","# new_state_h (after Dense(300)): [2, 1, 300]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"68oE-F-AQFw3"},"source":["*Multi-head Attention*"]},{"cell_type":"code","metadata":{"id":"YFDwQjGbSvs3"},"source":["if model_architecture == 'bilstmmultiatt':\n","    sequence_input = Input(shape=(max_seq_len,), dtype='int32', name='input')\n","    embedding_layer = Embedding(emb_matrix_shape[0],\n","                                input_emb_size,\n","                                weights=[embedding_matrix] if 'embedding_matrix' in globals() else [np.zeros(emb_matrix_shape)],\n","                                input_length=max_seq_len,\n","                                trainable=not fixed_embeddings,\n","                                mask_zero=True, name='embedding')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(lstm_output_size, return_sequences=True, return_state=True, name='lstm'))(embedded_sequences)\n","    state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n","    state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n","    state_h = tf.expand_dims(state_h, 1)\n","    context_vector = MultiheadAttention(num_heads, d_model)([state_h, lstm, lstm])\n","    output = Dense(dense_output_size, activation='tanh', name='dense')(context_vector)\n","    output = tf.squeeze(output)\n","    adadelta = optimizers.Adadelta(lr=learning_rate)\n","    model = Model(inputs=sequence_input, outputs=output)\n","    if loss_function == 'cosine':\n","        model.compile(loss=cosine_loss,\n","                    optimizer=adadelta,metrics=[cosine_sim])\n","    elif loss_function == 'ranking':\n","        model.compile(loss=rank_loss(margin),\n","                    optimizer=adadelta,metrics=[cosine_sim])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6XAA9tjEwT09"},"source":["if experiment_phase == 'training':\n","    with open(log_dir,'a+',encoding='utf-8') as file:\n","        model.summary(print_fn=lambda line: file.write(line + '\\n'))\n","    write_line_to_file(str(model.loss), log_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VtLV5GUxUsOW"},"source":["if experiment_phase in {'training','continue'}:\n","    json_logging_callback = LambdaCallback(on_epoch_end=lambda epoch, logs:e_end('cosine_sim', epoch, logs, project_path+folder_path+'report.dat'))\n","    optimizer_weights_callback = LambdaCallback(on_epoch_end=save_optimizer_state(model, project_path+folder_path+'optimizer_weights.pkl'))\n","    model_weights_checkpoint = ModelCheckpoint(project_path+folder_path+'weights.h5', monitor='val_loss', verbose=0,save_best_only=True, mode='min', period=1, save_weights_only=True)\n","    early_stopping = EarlyStopping(monitor='val_loss', restore_best_weights=True,patience=2, mode='min',min_delta=0.001)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RWDiArvXU4sL"},"source":["if experiment_phase == 'continue':\n","    model.fit(x_train[:1000], y_train[:1000], epochs=1, batch_size=batch_size, validation_data=(x_dev, y_dev))\n","    with open(project_path+folder_path+'optimizer_weights.pkl') as f:\n","        optimizer_weights = pickle.load(f)\n","    model.optimizer.set_weights(optimizer_weights)   \n","    model.set_weights(project_path+folder_path+'weights.h5')\n","    start = time()\n","    history = model.fit(x_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(x_dev, y_dev)\n","    ,callbacks=[model_weights_checkpoint, early_stopping, json_logging_callback, optimizer_weights_callback]\n","    )   \n","    end = time()\n","    duration = round(end-start,2)\n","    write_line_to_file('Training Duration: '+str(duration)+' Seconds', log_dir)\n","elif experiment_phase == 'training':\n","    start = time()\n","    history = model.fit(x_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(x_dev, y_dev)\n","    ,callbacks=[model_weights_checkpoint, early_stopping, json_logging_callback, optimizer_weights_callback]\n","    )   \n","    end = time()\n","    duration = round(end-start,2)\n","    write_line_to_file('Training Duration: '+str(duration)+' Seconds', log_dir)\n","elif experiment_phase == 'querying':\n","    model.load_weights(project_path+folder_path+'weights.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HvCSOfTFPRWw"},"source":["# model.optimizer.set_weights(weight_values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r6Jx4uwK67RB"},"source":["if experiment_phase in {'training','continue'}:\n","    x_test = array([item['array'] for item in test])\n","    y_test = array([comparison_matrix[h2id[item['word']]] if item['word'] in h2id else output_vec_model[item['word']] for item in test])\n","    test_eval = model.evaluate(x_test, y_test)\n","    test_loss = test_eval[0]\n","    test_cosim = test_eval[1]\n","    log_str = 'Test Loss: '+str(test_loss)+' - '+'Cosine Similarity: ' + str(test_cosim)\n","    write_line_to_file(log_str, log_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lt0qyeJDha_J"},"source":["if experiment_phase in {'training','continue'}:\n","    plt.plot(history.history['cosine_sim'])\n","    plt.plot(history.history['val_cosine_sim'])\n","    plt.title('Training History')\n","    plt.ylabel('Cosine Similarity')\n","    plt.xlabel('epoch')\n","    plt.legend(['training', 'development'], loc='upper left')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nTtfuWPQaXQP"},"source":["if experiment_phase in {'training','continue'}:\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('Training History')\n","    plt.ylabel('Loss')\n","    plt.xlabel('epoch')\n","    plt.legend(['training', 'development'], loc='upper left')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v1NVpDTFX4MC"},"source":["**Evaluating the model based on its performance on the testing set**"]},{"cell_type":"code","metadata":{"id":"9uagVHoMKs49"},"source":["if experiment_phase in {'training','continue'}:\n","    train_samples = stratified_sample(train, {'amid','moeen-vy','dehkhoda-vy'}, num_head_words, acc_eval_sample_Size)\n","    train_inputs = {\n","        'definitions':[item['original_definition'] for item in train_samples],\n","        'words':[item['word'] for item in train_samples],\n","        'sources':[item['source'] for item in train_samples],\n","        'sense_tags':[item['sense_tags'] for item in train_samples],\n","        'general_tags':[item['general_tags'] for item in train_samples]\n","    }\n","    if 'attention_model' in globals():\n","        q = query(train_inputs, result_topn, max_seq_len, {'model':model, 'attention_model':attention_model}, tools)\n","    else:\n","        q = query(train_inputs, result_topn, max_seq_len, {'model':model}, tools)\n","    if use_intent_classifier == True:\n","        write_line_to_file('-------------', log_dir)\n","        intent_eval = evaluate_intent_classifier(q, word_tags)\n","        write_line_to_file('Intent Classifier Seen Evaluation: ', log_dir)\n","        write_line_to_file(str(intent_eval), log_dir)\n","    e = evaluate(q)\n","    write_line_to_file('-------------', log_dir)\n","    log_str = 'Seen Evaluation:\\n'+'Median: '+str(e['main_eval']['median'])+', Variance: '+str(e['main_eval']['variance'])+', Acc@10: '+str(e['main_eval']['acc@10'])+', Acc@100: '+str(e['main_eval']['acc@100'])+'\\n'\n","    write_line_to_file(log_str, log_dir)\n","    log_str = 'Seen Synonym Evaluation:\\n'+'Median: '+str(e['synset_eval']['median'])+', Variance: '+str(e['synset_eval']['variance'])+', Acc@10: '+str(e['synset_eval']['acc@10'])+', Acc@100: '+str(e['synset_eval']['acc@100'])+'\\n'\n","    write_line_to_file(log_str, log_dir)\n","    write_line_to_file(str(e['bad_results']), log_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DkFrpE5Vzm7w"},"source":["if experiment_phase in {'training','continue'}:\n","    test_samples = stratified_sample(test, {'amid','moeen-vy','dehkhoda-vy'}, num_head_words, acc_eval_sample_Size)\n","    test_inputs = {\n","        'definitions':[item['original_definition'] for item in test_samples],\n","        'words':[item['word'] for item in test_samples],\n","        'sources':[item['source'] for item in test_samples],\n","        'sense_tags':[item['sense_tags'] for item in test_samples],\n","        'general_tags':[item['general_tags'] for item in test_samples]\n","    }\n","    if 'attention_model' in globals():\n","        q = query(test_inputs, result_topn, max_seq_len, {'model':model, 'attention_model':attention_model}, tools)\n","    else:\n","        q = query(test_inputs, result_topn, max_seq_len, {'model':model}, tools)\n","    if use_intent_classifier == True:\n","        write_line_to_file('-------------', log_dir)\n","        intent_eval = evaluate_intent_classifier(q, word_tags)\n","        write_line_to_file('Intent Classifier Unseen Evaluation: ', log_dir)\n","        write_line_to_file(str(intent_eval), log_dir)\n","    e = evaluate(q)\n","    write_line_to_file('-------------', log_dir)\n","    log_str = 'Unseen Evaluation:\\n'+'Median: '+str(e['main_eval']['median'])+', Variance: '+str(e['main_eval']['variance'])+', Acc@10: '+str(e['main_eval']['acc@10'])+', Acc@100: '+str(e['main_eval']['acc@100'])+'\\n'\n","    write_line_to_file(log_str, log_dir)\n","    log_str = 'Unseen Synonym Evaluation:\\n'+'Median: '+str(e['synset_eval']['median'])+', Variance: '+str(e['synset_eval']['variance'])+', Acc@10: '+str(e['synset_eval']['acc@10'])+', Acc@100: '+str(e['synset_eval']['acc@100'])+'\\n'\n","    write_line_to_file(log_str, log_dir)\n","    write_line_to_file(str(e['bad_results']), log_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y9TwgJ7hFYIv"},"source":["if experiment_phase in {'training','continue'}:\n","    synset_good10 = [item for item in q if 1<=item['synset_word_rank']<=10]\n","    synset_good100 = [item for item in q if 10<item['synset_word_rank']<=100]\n","    synset_bad100 = [item for item in q if 100<item['synset_word_rank']]\n","    save_group_reports(synset_good10, project_path+folder_path+'good10.html')\n","    save_group_reports(synset_good100, project_path+folder_path+'good100.html')\n","    save_group_reports(synset_bad100, project_path+folder_path+'bad100.html')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xA2Ixg6vevvl"},"source":["# if experiment_phase in {'training','continue'}:\n","#     dictionaries = {'amid','moeen-vy','dehkhoda-vy'}\n","#     for dictionary in dictionaries:\n","#         test_samples = stratified_sample(test, {dictionary}, num_head_words, mos_eval_sample_size)\n","#         test_inputs = {\n","#             'definitions':[item['original_definition'] for item in test_samples],\n","#             'words':[item['word'] for item in test_samples],\n","#             'sources':[item['source'] for item in test_samples]\n","#         }\n","#         if 'attention_model' in globals():\n","#             q = query(test_inputs, result_topn, max_seq_len, {'model':model, 'attention_model':attention_model}, tools)\n","#         else:\n","#             q = query(test_inputs, result_topn, max_seq_len, {'model':model}, tools)\n","#         e = evaluate(q)\n","#         write_line_to_file('-------------', log_dir)\n","#         log_str = 'Source: '+dictionary\n","#         write_line_to_file(log_str, log_dir)\n","#         log_str = 'Unseen Evaluation:\\n'+'Median: '+str(e['main_eval']['median'])+', Variance: '+str(e['main_eval']['variance'])+', Acc@10: '+str(e['main_eval']['acc@10'])+', Acc@100: '+str(e['main_eval']['acc@100'])+'\\n'\n","#         write_line_to_file(log_str, log_dir)\n","#         log_str = 'Unseen Synonym Evaluation:\\n'+'Median: '+str(e['synset_eval']['median'])+', Variance: '+str(e['synset_eval']['variance'])+', Acc@10: '+str(e['synset_eval']['acc@10'])+', Acc@100: '+str(e['synset_eval']['acc@100'])+'\\n'\n","#         write_line_to_file(log_str, log_dir)\n","#         write_line_to_file(str(e['bad_results']), log_dir)\n","#         correct_words = [item['main_word'] for item in q]\n","#         input_descriptions = [item['original_definition'] for item in q]\n","#         dict_evaluation_df = pd.DataFrame(list(zip(correct_words, input_descriptions)), columns=['word','description'])\n","#         dict_evaluation_df.to_excel(project_path+folder_path+dictionary+'.xlsx', encoding='utf-8')\n","#         for k in range(3):\n","#             output_words = [item['topwords'][k] for item in q]\n","#             evaluation_df = pd.DataFrame(list(zip(output_words, input_descriptions)), columns=['word','description'])\n","#             evaluation_df.to_excel(project_path+folder_path+dictionary+'-'+str(k+1)+'.xlsx', encoding='utf-8')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y_w7rs5WMu3Z","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601453319262,"user_tz":-210,"elapsed":21066,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"990864d5-2d3b-4573-f92c-cc940b17353a"},"source":["definition = '     '\n","inputs = {'definitions': [definition],\n","          'words':['UNK']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : UNK</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :      </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,92.83310920000076%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,64.94956612586975%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.86804411560297%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,94.34928558766842%)\">&nbsp&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :  --  --  --  --  --  --  --  --  -- </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">  UNK    20000  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">   (UNK)    20000  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"slNRt_mYC4Jf","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601453319264,"user_tz":-210,"elapsed":21053,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"39c5ad44-2818-43a6-a3b1-66f4a12fdc99"},"source":["definition = '     '\n","inputs = {'definitions': [definition],\n","          'words':['']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :      </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,68.68666708469391%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,95.13376019895077%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,86.17957383394241%)\">&nbsp&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :  --  --  --  --  --  --  --  --  -- </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">      4  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">   ()    4  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"eD___1-MC4a9","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601453370939,"user_tz":-210,"elapsed":1066,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"79ad6fd7-c8ad-4bd8-f1f7-2c53e1de31ef"},"source":["definition = '    '\n","inputs = {'definitions': [definition],\n","          'words':['']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :     </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,95.89298814535141%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,79.59437072277069%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,86.15503162145615%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,88.3576087653637%)\">&nbsp&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :  --  --  --  --  --  --  --  --  -- </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">      1  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">   ()    50  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"w1t-De8fD-me","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601454057950,"user_tz":-210,"elapsed":1016,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"f25b06a9-3ddb-44b8-9a45-6a5d4da336bd"},"source":["definition = '  '\n","inputs = {'definitions': [definition],\n","          'words':['']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :         </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,95.3709788620472%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,99.32372593320906%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,89.03937265276909%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,68.16522777080536%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,98.100695759058%)\">&nbsp&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :  --  --  --  --  --  --  --  --  -- </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">      9  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">   ()    9  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"Ld4mKJH4GeC2","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601454092604,"user_tz":-210,"elapsed":1225,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"6623b685-45d9-4e81-8bc8-fd922c5b0616"},"source":["definition = '    '\n","inputs = {'definitions': [definition],\n","          'words':['']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :     </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,99.64240251574665%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,99.02830645442009%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,98.87389652431011%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,75.9470745921135%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,76.50831937789917%)\">&nbspUNK&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :  --  --  --  --  --  --  --  --  -- </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">      10  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">   ()    10  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"20NysKZ4GNvh","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601454360417,"user_tz":-210,"elapsed":1712,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"ecd58120-bd1d-424b-f02a-8fcbe74d4499"},"source":["definition = '     '\n","inputs = {'definitions': [definition],\n","          'words':['']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :      </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,70.77159881591797%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,92.53187403082848%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,90.41079059243202%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,96.28573916852474%)\">&nbsp&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :  --  --  --  --  --  --  --  --  -- </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">      13  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">   ()    54  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"EQR98opMD-9A","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601454496388,"user_tz":-210,"elapsed":945,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"8868f1f7-ab4e-49e3-fccc-2ff995612d4d"},"source":["definition = '    '\n","inputs = {'definitions': [definition],\n","          'words':['']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :     </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,85.79045832157135%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,73.3140617609024%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,94.03576254844666%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,96.85971662402153%)\">&nbsp&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :  --  --  --  --  --  --  --  --  -- </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">      2  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">   ()    8771  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"N5DG92UCIQ9D","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601454909140,"user_tz":-210,"elapsed":1085,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"89995ec1-2ba9-498a-e527-74a7ee2b7286"},"source":["definition = '  '\n","inputs = {'definitions': [definition],\n","          'words':['']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :   </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,94.12750266492367%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,77.8690904378891%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,78.00340503454208%)\">&nbsp&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :  --  --  --  --  --  --  --  --  -- </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">      6  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">   ()    6  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"xpTzJafVJzTP","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601455062777,"user_tz":-210,"elapsed":1056,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"29dfd04a-356a-4383-d9a0-66066f194c55"},"source":["definition = '  '\n","inputs = {'definitions': [definition],\n","          'words':['']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :   </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.2596988081932%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,53.139424324035645%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,99.60087747313082%)\">&nbsp&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :  --  --  --  --  --  --  --  --  -- </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">      1  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">   ()    1  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"15ztcwiZKEEw","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601455801874,"user_tz":-210,"elapsed":922,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"cbeedc86-8410-4274-a759-db53f36b49e9"},"source":["definition = '   '\n","inputs = {'definitions': [definition],\n","          'words':['']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :    </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.03902043402195%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,93.12536790966988%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,92.33013167977333%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,67.50548183917999%)\">&nbsp&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :  --  --  --  --  --  --  --  --  -- </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">      88  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">   ()    88  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"vxazuuslNgAn","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601455932224,"user_tz":-210,"elapsed":973,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"d1b90f79-9ea1-4284-e695-ff5cdc49d8fb"},"source":["definition = '    '\n","inputs = {'definitions': [definition],\n","          'words':['']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :     </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,55.956435203552246%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.56818972527981%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,96.47537395358086%)\">&nbsp&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :  --  --  --  --  --  --  --  --  -- </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">      115  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">   ()    115  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"4HYxcILyNj8W","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601456313882,"user_tz":-210,"elapsed":1094,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"1d18e38c-3915-4a59-f585-031f77344224"},"source":["definition = '           '\n","inputs = {'definitions': [definition],\n","          'words':['']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :            </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,82.583187520504%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,76.65493041276932%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,98.9894611760974%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.33178932219744%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,96.7645525932312%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,98.47671333700418%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,99.71616039983928%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,99.87163743935525%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,99.61157026700675%)\">&nbsp&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :  --  --  --  --  --  --  --  --  -- </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">      2  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">   ()    2  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"F7v4EiKHPa3T","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601457002851,"user_tz":-210,"elapsed":843,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"14a43514-cfc5-4342-b4cd-ac056c8a915a"},"source":["definition = '     '\n","inputs = {'definitions': [definition],\n","          'words':['UNK']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : UNK</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :      </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,98.37066400796175%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,84.82122868299484%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,89.4108884036541%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,77.39721983671188%)\">&nbsp&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :  --  --  --  --  --  --  --  --  -- </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">  UNK    20000  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">   (UNK)    20000  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"f9JTsU1bRpWW","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1601457931973,"user_tz":-210,"elapsed":1066,"user":{"displayName":"Arman Malekzadeh Lashkaryani","photoUrl":"","userId":"10793406456586908937"}},"outputId":"c71b55f1-3b04-418a-93de-f8c7dca90384"},"source":["definition = '      '\n","inputs = {'definitions': [definition],\n","          'words':['']}\n","models = {'model':model}\n","if 'attention_model' in globals():\n","    models['attention_model'] = attention_model\n","q = query(inputs, result_topn, max_seq_len, models, tools)\n","group_print_report([item for item in q])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :       </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : <div style=\"text-align:right;direction:rtl\"><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,98.34479372948408%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,68.16150546073914%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,89.14517983794212%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,97.74899743497372%)\">&nbsp&nbsp</span><span style=\"text-shadow: 0px 0px 5px #000000; white-space:nowrap;color: white; background-color:hsl(0,100%,96.5995229780674%)\">&nbsp&nbsp</span></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> : </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\"> :  --  --  --  --  --  --  --  --  -- </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">      1  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">   ()    1  </div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div style=\"text-align:right; direction:rtl;font-family:tahoma\">---------------------------------------------------------</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"8cBjkutkSGfF"},"source":[""],"execution_count":null,"outputs":[]}]}